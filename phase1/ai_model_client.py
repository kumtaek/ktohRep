#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ê³µí†µ ì¸í„°í˜ì´ìŠ¤
- ê°œë°œí™˜ê²½: Ollama (gemma3:1b)
- ìš´ì˜í™˜ê²½: vLLM (Qwen2.5 7B)
- ëª¨ë“ˆ êµì²´ ê°€ëŠ¥í•œ êµ¬ì¡°
"""

import os
import json
import logging
import requests
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    name: str
    url: str
    timeout: int = 30
    max_tokens: int = 2000
    temperature: float = 0.7

class AIModelClient(ABC):
    """AI ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        pass

class OllamaClient(AIModelClient):
    """Ollama í´ë¼ì´ì–¸íŠ¸ (ê°œë°œí™˜ê²½)"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model_name = config.name
        self.url = config.url
        self.timeout = config.timeout
    
    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """Ollama APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            url = f"{self.url}/api/generate"
            data = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get('temperature', 0.7),
                    "num_predict": kwargs.get('max_tokens', 2000)
                }
            }
            
            logger.info(f"ğŸš€ Ollama í˜¸ì¶œ ì‹œì‘: {self.model_name}")
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            logger.info(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {self.timeout}ì´ˆ")
            logger.debug(f"ğŸ“ ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:\n{prompt}")
            logger.debug(f"ğŸ“¤ ìš”ì²­ ë°ì´í„°: {json.dumps(data, ensure_ascii=False, indent=2)}")
            
            response = requests.post(url, json=data, timeout=self.timeout)
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '')
                logger.info(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {len(response_text)}ì")
                logger.info(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:100]}...")
                logger.debug(f"ğŸ“¥ ì „ì²´ ì‘ë‹µ ë‚´ìš©:\n{response_text}")
                logger.debug(f"ğŸ“¥ ì‘ë‹µ ë©”íƒ€ë°ì´í„°: {json.dumps(result, ensure_ascii=False, indent=2)}")
                return response_text
            else:
                logger.error(f"âŒ Ollama API ì˜¤ë¥˜: {response.status_code}")
                logger.error(f"ğŸ“„ ì˜¤ë¥˜ ë‚´ìš©: {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            logger.error(f"ğŸ” ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None
    
    def is_available(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            url = f"{self.url}/api/tags"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model.get('name', '') for model in models]
                is_available = self.model_name in model_names
                logger.info(f"Ollama ëª¨ë¸ {self.model_name} ì‚¬ìš© ê°€ëŠ¥: {is_available}")
                return is_available
            return False
        except Exception as e:
            logger.warning(f"Ollama ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

class VLLMClient(AIModelClient):
    """vLLM í´ë¼ì´ì–¸íŠ¸ (ìš´ì˜í™˜ê²½)"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model_name = config.name
        self.url = config.url
        self.timeout = config.timeout
    
    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """vLLM APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            url = f"{self.url}/v1/chat/completions"
            data = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
                "temperature": kwargs.get('temperature', self.config.temperature),
                "stream": False
            }
            
            logger.info(f"ğŸš€ vLLM í˜¸ì¶œ ì‹œì‘: {self.model_name}")
            logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            logger.info(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {self.timeout}ì´ˆ")
            logger.debug(f"ğŸ“ ì „ì†¡í•  í”„ë¡¬í”„íŠ¸:\n{prompt}")
            logger.debug(f"ğŸ“¤ ìš”ì²­ ë°ì´í„°: {json.dumps(data, ensure_ascii=False, indent=2)}")
            
            response = requests.post(url, json=data, timeout=self.timeout)
            
            if response.status_code == 200:
                result = response.json()
                response_text = result['choices'][0]['message']['content']
                logger.info(f"âœ… vLLM ì‘ë‹µ ì™„ë£Œ: {len(response_text)}ì")
                logger.info(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:100]}...")
                logger.debug(f"ğŸ“¥ ì „ì²´ ì‘ë‹µ ë‚´ìš©:\n{response_text}")
                logger.debug(f"ğŸ“¥ ì‘ë‹µ ë©”íƒ€ë°ì´í„°: {json.dumps(result, ensure_ascii=False, indent=2)}")
                return response_text
            else:
                logger.error(f"vLLM API ì˜¤ë¥˜: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"vLLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def is_available(self) -> bool:
        """vLLM ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            url = f"{self.url}/v1/models"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                models = response.json().get('data', [])
                model_names = [model.get('id', '') for model in models]
                is_available = self.model_name in model_names
                logger.info(f"vLLM ëª¨ë¸ {self.model_name} ì‚¬ìš© ê°€ëŠ¥: {is_available}")
                return is_available
            return False
        except Exception as e:
            logger.warning(f"vLLM ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

class AIModelManager:
    """AI ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self, config_path: str = "config/ai_config.yaml"):
        self.config = self._load_config(config_path)
        self.clients = {}
        self._initialize_clients()
    
    def _load_config(self, config_path: str) -> dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except:
            # ê¸°ë³¸ ì„¤ì •
            return {
                'environment': 'development',  # development, production
                'models': {
                    'ollama': {
                        'name': 'gemma3:1b',
                        'url': 'http://localhost:11434',
                        'timeout': 30
                    },
                    'vllm': {
                        'name': 'qwen2.5-7b',
                        'url': 'http://localhost:8000',
                        'timeout': 60
                    }
                }
            }
    
    def _initialize_clients(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # Ollama í´ë¼ì´ì–¸íŠ¸
        ollama_config = self.config.get('models', {}).get('ollama', {})
        ollama_model_config = ModelConfig(
            name=ollama_config.get('name', 'gemma3:1b'),
            url=ollama_config.get('url', 'http://localhost:11434'),
            timeout=ollama_config.get('timeout', 30)
        )
        self.clients['ollama'] = OllamaClient(ollama_model_config)
        
        # vLLM í´ë¼ì´ì–¸íŠ¸
        vllm_config = self.config.get('models', {}).get('vllm', {})
        vllm_model_config = ModelConfig(
            name=vllm_config.get('name', 'qwen2.5-7b'),
            url=vllm_config.get('url', 'http://localhost:8000'),
            timeout=vllm_config.get('timeout', 60)
        )
        self.clients['vllm'] = VLLMClient(vllm_model_config)
    
    def generate(self, prompt: str, **kwargs) -> Optional[str]:
        """í…ìŠ¤íŠ¸ ìƒì„± (í™˜ê²½ì— ë”°ë¼ ìë™ ì„ íƒ)"""
        environment = self.config.get('environment', 'development')
        
        if environment == 'development':
            # ê°œë°œí™˜ê²½: Ollama ìš°ì„  ì‹œë„
            if self.clients['ollama'].is_available():
                logger.info("ê°œë°œí™˜ê²½: Ollama ì‚¬ìš©")
                return self.clients['ollama'].generate(prompt, **kwargs)
            elif self.clients['vllm'].is_available():
                logger.info("ê°œë°œí™˜ê²½: vLLM ë°±ì—… ì‚¬ìš©")
                return self.clients['vllm'].generate(prompt, **kwargs)
        else:
            # ìš´ì˜í™˜ê²½: vLLM ìš°ì„  ì‹œë„
            if self.clients['vllm'].is_available():
                logger.info("ìš´ì˜í™˜ê²½: vLLM ì‚¬ìš©")
                return self.clients['vllm'].generate(prompt, **kwargs)
            elif self.clients['ollama'].is_available():
                logger.info("ìš´ì˜í™˜ê²½: Ollama ë°±ì—… ì‚¬ìš©")
                return self.clients['ollama'].generate(prompt, **kwargs)
        
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    def get_available_models(self) -> Dict[str, bool]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return {
            'ollama': self.clients['ollama'].is_available(),
            'vllm': self.clients['vllm'].is_available()
        }
    
    def switch_environment(self, environment: str):
        """í™˜ê²½ ì „í™˜"""
        if environment in ['development', 'production']:
            self.config['environment'] = environment
            logger.info(f"í™˜ê²½ ì „í™˜: {environment}")
        else:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í™˜ê²½: {environment}")

# í¸ì˜ í•¨ìˆ˜
def create_ai_client(environment: str = 'development') -> AIModelManager:
    """AI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    manager = AIModelManager()
    manager.switch_environment(environment)
    return manager

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    client = create_ai_client('development')
    
    print("=== AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ===")
    available = client.get_available_models()
    for model, is_available in available.items():
        print(f"{model}: {'âœ…' if is_available else 'âŒ'}")
    
    if any(available.values()):
        print("\n=== AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        response = client.generate("Hello, how are you?")
        if response:
            print(f"ì‘ë‹µ: {response[:100]}...")
        else:
            print("ì‘ë‹µ ì‹¤íŒ¨")
    else:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
