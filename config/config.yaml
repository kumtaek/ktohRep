# Source Analyzer Configuration
# 이 파일은 Source Analyzer의 모든 설정 및 동작 방식을 정의합니다.
# 모든 설정은 이 파일을 통해 관리되며, 환경 변수 사용은 권장되지 않습니다.

# Phase 1: Metadata Generation Configuration

# 데이터베이스 설정
# Source Analyzer가 분석 결과를 저장하고 관리하는 데 사용하는 데이터베이스를 설정합니다.
database:
  type: sqlite  # 사용하고자 하는 데이터베이스 유형 (sqlite, oracle)
  sqlite:
    path: "./data/metadata.db"  # SQLite 데이터베이스 파일 경로
    wal_mode: true              # WAL(Write-Ahead Logging) 모드 사용 여부 (성능 향상)
  oracle:
    host: "localhost"           # Oracle 데이터베이스 호스트
    port: 1521                  # Oracle 데이터베이스 포트
    service_name: "orcl"        # Oracle 서비스 이름
    username: "analyzer"        # Oracle 사용자 이름
    password: "password"        # Oracle 비밀번호

# 벡터 스토어 설정
# LLM 기반 분석을 위한 벡터 임베딩 저장소를 설정합니다.
vector_store:
  type: faiss  # 사용하고자 하는 벡터 스토어 유형 (현재 faiss만 지원)
  faiss:
    index_path: "./data/faiss.index"  # FAISS 인덱스 파일 경로
    model_name: "BAAI/bge-m3"         # 임베딩 모델 이름 (다국어 지원)
    dimension: 1024                   # 임베딩 벡터 차원

# 파서 설정
# 다양한 언어 및 파일 유형에 대한 파서 활성화 및 유형을 설정합니다.
parsers:
  java:
    enabled: true          # Java 파서 활성화 여부
    parser_type: "javaparser"  # Java 파서 유형 (javaparser 또는 tree-sitter)
    java_version: 8        # 분석할 Java 소스 코드 버전
  jsp:
    enabled: true          # JSP 파서 활성화 여부
    parser_type: "antlr"   # JSP 파서 유형 (현재 antlr만 지원)
  mybatis:
    enabled: true          # MyBatis 파서 활성화 여부
    parser_type: "jsqlparser" # MyBatis SQL 파서 유형 (현재 jsqlparser만 지원)
  sql:
    enabled: true          # 일반 SQL 파서 활성화 여부
    parser_type: "jsqlparser" # SQL 파서 유형 (현재 jsqlparser만 지원)
    oracle_dialect: true   # Oracle SQL 방언 사용 여부
  tree_sitter:
    enabled: false         # Tree-sitter 파서 활성화 여부 (향후 기능)
    languages: ["java", "javascript", "typescript", "python"] # Tree-sitter로 분석할 언어 목록

# LLM 설정 (메타데이터 보강용)
# 코드 메타데이터 보강에 사용될 대규모 언어 모델(LLM)의 설정을 정의합니다.
llm:
  enabled: true             # LLM 기반 보강 활성화 여부
  base_model: "qwen2.5-7b"  # 기본 LLM 모델 이름
  fallback_model: "qwen2.5-32b" # 기본 모델 실패 시 사용할 대체 LLM 모델 이름
  vllm_endpoint: "http://localhost:8000/v1" # vLLM 서버 엔드포인트 URL
  temperature: 0.0          # LLM 응답의 무작위성 (0.0은 가장 결정론적)
  max_tokens: 2048          # LLM 응답의 최대 토큰 수
  model_quality: 0.6        # LLM 모델 품질 임계값 (0.0 ~ 1.0)
  provider: "ollama"        # LLM 제공자 (auto, ollama, vllm, openai)

# 임베딩 모델 설정
# 텍스트를 벡터로 변환하는 데 사용될 임베딩 모델 목록을 정의합니다.
embedding:
  models:
    - name: "BAAI/bge-m3"  # 다국어 임베딩 모델
      type: "multilingual"
      max_length: 8192
      batch_size: 32
    - name: "jhgan00/ko-sentence-transformers" # 한국어 임베딩 모델
      type: "korean"
      max_length: 512
      batch_size: 16

# 처리 설정
# 코드 분석 및 LLM 처리의 일반적인 설정을 정의합니다.
processing:
  max_workers: 4            # 병렬 처리 작업자 수
  chunk_size: 512           # 텍스트 처리 단위 (토큰 또는 문자)
  chunk_overlap: 50         # 텍스트 청크 간의 중복 크기
  confidence_threshold: 0.5 # 분석 결과의 신뢰도 임계값 (0.0 ~ 1.0)

# 출력 설정
# 분석 결과 및 메타데이터 출력 방식을 설정합니다.
output:
  metadata_only: true       # 소스 코드 저장 없이 메타데이터만 저장할지 여부
  include_line_ranges: true # 메타데이터에 코드 라인 범위 포함 여부
  include_confidence: true  # 메타데이터에 신뢰도 점수 포함 여부
  log_enrichment: true      # LLM 보강 로그 기록 여부

# 파일 패턴 설정
# 분석에 포함하거나 제외할 파일 및 디렉토리 패턴을 정의합니다.
file_patterns:
  include:
    - "**/*.java"
    - "**/*.jsp"
    - "**/*.xml"
    - "**/*.properties"
  exclude:
    - "**/target/**"
    - "**/build/**"
    - "**/test/**"
    - "**/.git/**"

# DB 스키마 파일 설정
# 데이터베이스 스키마 분석에 필요한 파일 목록 및 경로 템플릿을 정의합니다.
db_schema:
  required_files:
    - "ALL_TABLES.csv"
    - "ALL_TAB_COLUMNS.csv"
    - "PK_INFO.csv"
  path_template: "./PROJECT/{project_name}/DB_SCHEMA/" # DB 스키마 파일 경로 템플릿

# 로깅 설정
# 애플리케이션의 로깅 동작을 설정합니다.
logging:
  level: "DEBUG"            # 로깅 레벨 (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  file: "./logs/analyzer.log" # 로그 파일 경로
  max_size: "100MB"         # 로그 파일 최대 크기
  backup_count: 3           # 보관할 백업 로그 파일 수

# 웹 서버 (백엔드) 설정
# 웹 대시보드 백엔드 서버의 동작을 설정합니다.
server:
  host: "127.0.0.1"         # 서버가 바인딩될 IP 주소
  port: 8000                # 서버가 수신 대기할 포트 번호
  debug: true               # 디버그 모드 활성화 여부
  reload: false             # 코드 변경 시 서버 자동 재시작 여부 (개발 환경에서 유용)
  api_prefix: "/api"        # API 엔드포인트 접두사
  cors:                     # CORS(Cross-Origin Resource Sharing) 설정
    enabled: true           # CORS 활성화 여부
    allow_origins: ["*"]    # 허용할 Origin 목록 (예: ["http://localhost:3000", "http://127.0.0.1:3000"])
    allow_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"] # 허용할 HTTP 메서드
    allow_headers: ["*"]    # 허용할 HTTP 헤더
    credentials: false      # 자격 증명(쿠키, HTTP 인증) 허용 여부
    expose_headers: []      # 클라이언트에 노출할 헤더 목록
  log_file: "./logs/backend.log" # 백엔드 로그 파일 경로 (생략 시 상위 로깅 설정 따름)
  log_level: "DEBUG"        # 백엔드 로깅 레벨 (생략 시 상위 로깅 설정 따름)
  static:                   # 정적 파일 서빙 설정
    enabled: false          # 정적 파일 서빙 활성화 여부
    folder: "./web-dashboard/frontend/dist" # 정적 파일이 위치한 폴더 경로
    url_path: "/static"     # 정적 파일에 접근할 URL 경로

# LLM Assist (Phase1) — LLM 기반 보강 기능 설정
# LLM을 활용한 코드 메타데이터 보강 기능의 세부 설정을 정의합니다.
llm_assist:
  enabled: true             # LLM 보강 기능 활성화 여부
  provider: "auto"          # LLM 제공자 (auto, ollama, vllm, openai)
  low_conf_threshold: 0.6   # 낮은 신뢰도 임계값
  max_calls_per_run: 50     # 한 번 실행 시 LLM 호출 최대 횟수
  file_max_lines: 1200      # LLM이 처리할 파일의 최대 라인 수
  temperature: 0.0          # LLM 응답의 무작위성
  max_tokens: 512           # LLM 응답의 최대 토큰 수
  strict_json: true         # LLM 응답이 엄격한 JSON 형식이어야 하는지 여부
  cache: true               # LLM 응답 캐싱 활성화 여부
  cache_dir: ./out/llm_cache # LLM 캐시 디렉토리 경로
  log_prompt: false         # LLM 프롬프트 로깅 활성화 여부
  dry_run: false            # 실제 LLM 호출 없이 드라이 런 모드 활성화 여부
  fallback_to_ollama: true  # vLLM 실패 시 Ollama로 대체할지 여부
  enrich:                   # LLM을 통한 보강 항목별 설정
    table_comments:
      enabled: true
      max_tables: 25
    column_comments:
      enabled: true
      max_columns: 50
    sql_summary:
      enabled: true
      max_items: 30
    method_summary:
      enabled: true
      max_items: 30
    jsp_summary:
      enabled: true
      max_items: 30
    join_infer:
      enabled: true
      max_items: 50
    edge_hint:
      enabled: true
      max_items: 50
  # LLM 제공자별 상세 설정 (환경 변수 대신 이 파일에서 직접 설정)
  vllm_base_url: "http://localhost:8000/v1" # vLLM 서버 기본 URL
  vllm_api_key: "EMPTY"                     # vLLM API 키
  vllm_model: "Qwen2.5"                     # vLLM 모델 이름
  ollama_host: "http://localhost:11434"     # Ollama 서버 호스트
  ollama_model: "gemma3:1b"                 # Ollama 모델 이름