







설계 개선 검토

  기존 설계는 모든 엔티티 쌍(N*N)에 대해 반복적으로 계산을 수행하여 대규모 프로젝트에서는 성능 저하가 우려됩니다. 또한, 여러 연관성
  근거(reason)에 대한 점수를 어떻게 통합할지에 대한 정책이 구체적이지 않습니다.

  개선 아이디어:

1. 계산 효율성 향상: N*N 전체 비교를 피하고, 각 연관성 규칙의 특성에 맞는 최적화된 방식으로 계산을 수행합니다.
   
   * 디렉토리 근접성: 전체 파일을 순회하는 대신, 디렉토리별로 파일을 그룹화한 후 그룹 내에서만 비교합니다.
   * FK 관계 / 함수 호출: 이미 Edge 테이블에 저장된 관계이므로, 모든 노드 쌍을 비교할 필요 없이 Edge 정보를 직접 변환하여 높은 점수를
     부여합니다.

2. 점수 통합 정책 구체화: 여러 연관성 근거가 발견될 경우, 가장 높은 점수를 해당 노드 쌍의 최종 연관성 점수로 채택합니다. 이는 가장 명확하고
   강한 연결 고리를 우선시하는 직관적인 방식입니다.

3. 모듈화된 계산기: 각 연관성 계산 로직을 별도의 '전략' 함수로 분리하여 향후 새로운 규칙을 추가하기 용이한 구조로 설계합니다.
   
   이 개선안을 반영하여 새로운 설계 문서(연관성구현설계_v2.md)를 작성하겠습니다.



# 연관성 메타데이터 구축 상세 설계 (v2)

## 1. 개요

기존 설계를 검토하고, 계산 효율성 및 확장성을 개선한 연관성 메타데이터 구축 방안을 재설계한다. 주요 목표는 대규모 프로젝트에서도 효율적으로 동작하는 연관성 계산 파이프라인을 구축하는 것이다.

## 2. 데이터 모델

`연관성구현설계.md`에서 제안된 `Relatedness` 테이블 스키마는 유연성이 높아 그대로 유지한다. 이 테이블은 직접적인 `Edge`가 없는 엔티티 간의 관계도 표현할 수 있어 확장성에 유리하다.

## 3. 연관성 계산 파이프라인 (개선된 설계)

`itertools.combinations`를 사용한 `O(N^2)` 방식 대신, 각 연관성 규칙의 특성을 활용한 개별적인 계산 전략을 순차적으로 적용한다. 이를 통해 전체 계산량을 대폭 감소시킨다.

### 3.1. 메인 파이프라인 (`phase1/scripts/calculate_relatedness.py`)

계산기 클래스는 각 전략(strategy)을 순서대로 호출하는 역할을 한다.

```python
# phase1/scripts/calculate_relatedness.py (신규 생성)

from collections import defaultdict
from ..models.database import DatabaseManager, File, Class, DbTable, Edge # 등
from ..models.relatedness import Relatedness # 새로 추가될 모델

class RelatednessCalculator:
    def __init__(self, project_id):
        self.project_id = project_id
        self.dbm = DatabaseManager(...)
        self.session = self.dbm.get_session()
        # 각 노드 쌍에 대한 최고 점수를 저장: {(node1_key, node2_key): (score, reason)}
        self.relatedness_scores = defaultdict(lambda: (0.0, 'none'))

    def run(self):
        print("연관성 계산 시작...")
        # 각 전략을 순차적으로 실행
        self.apply_direct_edge_strategy()
        self.apply_directory_proximity_strategy()
        self.apply_naming_convention_strategy()
        # self.apply_semantic_strategy() # 향후 확장

        print("DB에 연관성 점수 저장 중...")
        self.store_scores_to_db()
        print("완료.")

    def _update_score(self, node1_key, node2_key, score, reason):
        """기존 점수보다 높을 경우에만 점수를 업데이트"""
        # 순서를 보장하여 키를 생성 (e.g., ('file:1', 'class:3'))
        key = tuple(sorted((node1_key, node2_key)))

        current_score, _ = self.relatedness_scores[key]
        if score > current_score:
            self.relatedness_scores[key] = (score, reason)

    def store_scores_to_db(self):
        # self.relatedness_scores에 저장된 최종 점수들을 Relatedness 테이블에 bulk insert/update
        pass

    # --- 각 전략(Strategy) 메소드들 ---
    def apply_direct_edge_strategy(self):
        pass
    def apply_directory_proximity_strategy(self):
        pass
    def apply_naming_convention_strategy(self):
        pass
```

### 3.2. 계산 전략별 상세 설계

#### 1. 직접 연결(Edge) 기반 전략

`Edge` 테이블의 `call`, `use_table`, `fk` 등의 관계는 이미 분석된 명확한 연관성이므로, 이를 직접 변환하여 높은 점수를 부여한다. 이는 `O(E)` 복잡도(E: 엣지 수)로 매우 효율적이다.

```python
# RelatednessCalculator 클래스 내 메소드
def apply_direct_edge_strategy(self):
    print("  - 직접 연결(Edge) 기반 연관성 분석 중...")
    edges = self.session.query(Edge).filter(Edge.project_id == self.project_id).all()

    score_map = {
        'fk': 0.95,
        'inherits': 0.9,
        'implements': 0.9,
        'call': 0.7,
        'use_table': 0.75
    }

    for edge in edges:
        score = score_map.get(edge.edge_kind, 0.5) # 기타 엣지는 0.5점
        node1_key = f"{edge.source_type}:{edge.source_id}"
        node2_key = f"{edge.target_type}:{edge.target_id}"
        self._update_score(node1_key, node2_key, score, f"edge_{edge.edge_kind}")
```

#### 2. 디렉토리 근접성 전략

파일들을 디렉토리별로 그룹화한 후, 같은 디렉토리 내에 있는 파일 및 클래스/메소드 간에 점수를 부여한다. 이는 `O(D*F_avg^2)` 복잡도(D: 디렉토리 수, F_avg: 디렉토리당 평균 파일 수)로, 전체 `N^2` 비교보다 훨씬 효율적이다.

```python
# RelatednessCalculator 클래스 내 메소드
from itertools import combinations

def apply_directory_proximity_strategy(self):
    print("  - 디렉토리 근접성 기반 연관성 분석 중...")
    files_by_dir = defaultdict(list)
    all_files = self.session.query(File).filter(File.project_id == self.project_id).all()

    for file_obj in all_files:
        dir_name = os.path.dirname(file_obj.path)
        files_by_dir[dir_name].append(f"file:{file_obj.file_id}")

    for dir_name, nodes_in_dir in files_by_dir.items():
        # 같은 디렉토리에 있는 모든 노드 쌍에 대해 점수 부여
        for node1_key, node2_key in combinations(nodes_in_dir, 2):
            self._update_score(node1_key, node2_key, 0.6, 'directory_proximity')
```

#### 3. 이름 규칙(Naming Convention) 전략

`User`와 `UserService`, `UserController`처럼 이름에 규칙이 있는 경우 높은 연관성을 가진다. 전체 `N^2` 비교 대신, 특정 키워드(`Service`, `Controller`, `Repository` 등)를 중심으로 그룹화하여 비교 횟수를 줄인다.

```python
# RelatednessCalculator 클래스 내 메소드
from difflib import SequenceMatcher

def apply_naming_convention_strategy(self):
    print("  - 이름 규칙 기반 연관성 분석 중...")
    nodes = self._get_all_code_entities() # File, Class, Method 등

    # 베이스 이름(e.g., 'User')을 키로 노드들을 그룹화
    nodes_by_base_name = defaultdict(list)
    common_suffixes = ['Service', 'Controller', 'Repository', 'DAO', 'DTO', 'VO']
    for node in nodes:
        base_name = node['name']
        for suffix in common_suffixes:
            if base_name.endswith(suffix):
                base_name = base_name[:-len(suffix)]
                break
        nodes_by_base_name[base_name].append(node)

    for base_name, related_nodes in nodes_by_base_name.items():
        if len(related_nodes) > 1:
            for node1, node2 in combinations(related_nodes, 2):
                node1_key = f"{node1['type']}:{node1['id']}"
                node2_key = f"{node2['type']}:{node2['id']}"
                # 이름 규칙이 강력한 근거이므로 높은 점수 부여
                self._update_score(node1_key, node2_key, 0.8, 'naming_convention')
```

## 4. 결론

개선된 설계는 초기 설계의 `O(N^2)` 복잡도를 크게 낮추어 대규모 프로젝트에서도 실용적으로 연관성 메타데이터를 구축할 수 있게 한다. 또한, 각 연관성 판단 로직이 전략 메소드로 분리되어 있어 향후 새로운 규칙(e.g., AI 기반 의미 분석)을 추가하고 확장하기 용이한 구조이다. 다음 단계로 이 설계에 따라 `calculate_relatedness.py` 스크립트의 실제 구현을 진행한다.





 개선된 설계안 연관성구현설계_v2.md를 작성했습니다.

  이 새로운 설계는 기존의 모든 노드를 쌍으로 비교하는 방식 대신, 각 연관성의 특징을 활용하여 계산량을 대폭 줄이고 효율성을 높이는 데 중점을
  두었습니다.
