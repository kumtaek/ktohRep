"""
ì†ŒìŠ¤ ë¶„ì„ê¸° ë©”ì¸ ëª¨ë“ˆ
ë¦¬ë·° ì˜ê²¬ì„ ë°˜ì˜í•œ ê°œì„ ì‚¬í•­ë“¤ì´ ì ìš©ëœ 1ë‹¨ê³„ ë©”íƒ€ì •ë³´ ìƒì„± ì‹œìŠ¤í…œ
"""

import os
import sys
import argparse
import yaml
import asyncio
import hashlib
from pathlib import Path
from typing import Dict, Any, List
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€ (ìˆ˜ì •ë¨: srcì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ ì¶”ê°€)
project_root = Path(__file__).parent.parent  # srcì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬
sys.path.insert(0, str(project_root))

from src.models.database import DatabaseManager, File
from src.parsers.java_parser import JavaParser
from src.parsers.jsp_mybatis_parser import JspMybatisParser
from src.parsers.sql_parser import SqlParser
from src.database.metadata_engine import MetadataEngine
from src.utils.csv_loader import CsvLoader
from src.utils.logger import setup_logging, LoggerFactory, PerformanceLogger
from src.utils.confidence_calculator import ConfidenceCalculator
from src.utils.confidence_validator import ConfidenceValidator, ConfidenceCalibrator, GroundTruthEntry


class SourceAnalyzer:
    """ì†ŒìŠ¤ ë¶„ì„ê¸° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str):
        """
        ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.logger = setup_logging(self.config)
        
        self.logger.info("=" * 60)
        self.logger.info("ê°œì„ ëœ ì†ŒìŠ¤ ë¶„ì„ê¸° ì‹œì‘")
        self.logger.info("=" * 60)
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.db_manager = DatabaseManager(self.config)
            self.db_manager.initialize()
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            
            # íŒŒì„œë“¤ ì´ˆê¸°í™”
            self.parsers = self._initialize_parsers()
            self.logger.info(f"íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ: {list(self.parsers.keys())}")
            
            # ë©”íƒ€ë°ì´í„° ì—”ì§„ ì´ˆê¸°í™”
            self.metadata_engine = MetadataEngine(self.config, self.db_manager)
            
            # CSV ë¡œë” ì´ˆê¸°í™”
            self.csv_loader = CsvLoader(self.config)
            
            # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            self.confidence_calculator = ConfidenceCalculator(self.config)
            
            # Confidence Validation ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            self.confidence_validator = ConfidenceValidator(self.config, self.confidence_calculator)
            self.confidence_calibrator = ConfidenceCalibrator(self.confidence_validator)
            
            # ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ confidence formula ê²€ì¦ ì‹¤í–‰
            self._validate_confidence_formula_on_startup()
            
        except Exception as e:
            self.logger.critical("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨", exception=e)
            raise
            
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ (ê°œì„ ëœ ì˜ˆì™¸ ì²˜ë¦¬)"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                
            # ê¸°ë³¸ê°’ ì„¤ì •
            self._set_default_config(config)
            
            return config
            
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            raise
        except yaml.YAMLError as e:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise
            
    def _set_default_config(self, config: Dict[str, Any]):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì„¤ì •"""
        
        # ê¸°ë³¸ ì²˜ë¦¬ ì„¤ì •
        if 'processing' not in config:
            config['processing'] = {}
        processing = config['processing']
        processing.setdefault('max_workers', 4)
        processing.setdefault('batch_size', 10)
        processing.setdefault('chunk_size', 512)
        processing.setdefault('confidence_threshold', 0.5)
        
        # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
        if 'logging' not in config:
            config['logging'] = {}
        logging_config = config['logging']
        logging_config.setdefault('level', 'INFO')
        logging_config.setdefault('file', './logs/analyzer.log')
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ ì„¤ì •
        if 'confidence' not in config:
            config['confidence'] = {}
        confidence_config = config['confidence']
        confidence_config.setdefault('weights', {
            'ast': 0.4,
            'static': 0.3,
            'db_match': 0.2,
            'heuristic': 0.1
        })
        
    def _initialize_parsers(self) -> Dict[str, Any]:
        """íŒŒì„œë“¤ ì´ˆê¸°í™” (ê°œì„ ëœ ì˜ˆì™¸ ì²˜ë¦¬)"""
        parsers = {}
        
        try:
            # Java íŒŒì„œ
            if self.config.get('parsers', {}).get('java', {}).get('enabled', True):
                parsers['java'] = JavaParser(self.config)
                self.logger.info("Java íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
            # JSP/MyBatis íŒŒì„œ  
            if self.config.get('parsers', {}).get('jsp', {}).get('enabled', True):
                parsers['jsp_mybatis'] = JspMybatisParser(self.config)
                self.logger.info("JSP/MyBatis íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
            # SQL íŒŒì„œ
            if self.config.get('parsers', {}).get('sql', {}).get('enabled', True):
                parsers['sql'] = SqlParser(self.config)
                self.logger.info("SQL íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error("íŒŒì„œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜", exception=e)
            raise
            
        if not parsers:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        return parsers
        
    async def analyze_project(self, project_root: str, project_name: str = None, incremental: bool = False) -> Dict[str, Any]:
        """
        í”„ë¡œì íŠ¸ ì „ì²´ ë¶„ì„ (ê°œì„ ëœ ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            project_root: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„ (ì—†ìœ¼ë©´ í´ë”ëª… ì‚¬ìš©)
            incremental: ì¦ë¶„ ë¶„ì„ ëª¨ë“œ (ë³€ê²½ëœ íŒŒì¼ë§Œ ë¶„ì„)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        if not project_name:
            project_name = os.path.basename(project_root.rstrip('/\\'))
            
        start_time = time.time()
        
        with PerformanceLogger(self.logger, f"í”„ë¡œì íŠ¸ '{project_name}' ì „ì²´ ë¶„ì„"):
            
            try:
                # 1. í”„ë¡œì íŠ¸ ë“±ë¡
                self.logger.info(f"í”„ë¡œì íŠ¸ ë“±ë¡ ì¤‘: {project_name} ({project_root})")
                project_id = await self.metadata_engine.create_project(project_root, project_name)
                
                # 2. DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ
                await self._load_db_schema(project_root, project_name, project_id)
                
                # 3. ì†ŒìŠ¤ íŒŒì¼ë“¤ ìˆ˜ì§‘
                source_files = self._collect_source_files(project_root)
                self.logger.info(f"ë°œê²¬ëœ ì†ŒìŠ¤ íŒŒì¼: {len(source_files)}ê°œ")
                
                # ì¦ë¶„ ë¶„ì„ ëª¨ë“œì¸ ê²½ìš° ë³€ê²½ëœ íŒŒì¼ë§Œ ì„ ë³„
                if incremental:
                    source_files = await self._filter_changed_files(source_files, project_id)
                    self.logger.info(f"ì¦ë¶„ ë¶„ì„ ëŒ€ìƒ íŒŒì¼: {len(source_files)}ê°œ")
                
                if not source_files:
                    self.logger.warning("ë¶„ì„í•  ì†ŒìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return self._create_empty_result(project_id, project_name)
                
                # 4. íŒŒì¼ë“¤ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
                analysis_results = await self._analyze_files(source_files, project_id)
                
                # 5. ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
                self.logger.info("ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶• ì¤‘")
                await self.metadata_engine.build_dependency_graph(project_id)
                
                # 6. ë¶„ì„ ê²°ê³¼ ìš”ì•½
                summary = await self._generate_analysis_summary(project_id, analysis_results)
                
                total_time = time.time() - start_time
                self.logger.info(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ: {project_name} ({total_time:.2f}ì´ˆ)")
                
                return {
                    'project_id': project_id,
                    'project_name': project_name,
                    'project_root': project_root,
                    'analysis_time': total_time,
                    'files_analyzed': len(source_files),
                    'analysis_results': analysis_results,
                    'summary': summary
                }
                
            except Exception as e:
                self.logger.error(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {project_name}", exception=e)
                raise
                
    def _create_empty_result(self, project_id: int, project_name: str) -> Dict[str, Any]:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return {
            'project_id': project_id,
            'project_name': project_name,
            'analysis_time': 0,
            'files_analyzed': 0,
            'analysis_results': {
                'total_files': 0,
                'success_count': 0,
                'error_count': 0,
                'java_files': 0,
                'jsp_files': 0,
                'xml_files': 0,
                'total_classes': 0,
                'total_methods': 0,
                'total_sql_units': 0,
                'errors': []
            },
            'summary': {
                'basic_stats': {
                    'files': 0, 'classes': 0, 'methods': 0, 'sql_units': 0,
                    'joins': 0, 'filters': 0
                },
                'language_distribution': {},
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
        }
        
    async def _load_db_schema(self, project_root: str, project_name: str, project_id: int):
        """DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ (ê°œì„ ëœ ì˜¤ë¥˜ ì²˜ë¦¬)"""
        
        db_schema_path = os.path.join(project_root, "DB_SCHEMA")
        if not os.path.exists(db_schema_path):
            self.logger.warning(f"DB ìŠ¤í‚¤ë§ˆ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {db_schema_path}")
            return
            
        self.logger.info(f"DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë”© ì¤‘: {db_schema_path}")
        
        required_files = self.config['db_schema']['required_files']
        loaded_files = []
        
        for csv_file in required_files:
            csv_path = os.path.join(db_schema_path, csv_file)
            
            try:
                if os.path.exists(csv_path):
                    result = await self.csv_loader.load_csv(csv_path, project_id)
                    loaded_files.append(csv_file)
                    self.logger.info(f"CSV ë¡œë“œ ì™„ë£Œ: {csv_file} ({result['records']}ê±´)")
                else:
                    self.logger.warning(f"CSV íŒŒì¼ ì—†ìŒ: {csv_path}")
                    
            except Exception as e:
                self.logger.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {csv_file}", exception=e)
                
        if loaded_files:
            self.logger.info(f"DB ìŠ¤í‚¤ë§ˆ ë¡œë”© ì™„ë£Œ: {len(loaded_files)}/{len(required_files)} íŒŒì¼")
        else:
            self.logger.warning("DB ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    def _collect_source_files(self, project_root: str) -> List[str]:
        """ì†ŒìŠ¤ íŒŒì¼ë“¤ ìˆ˜ì§‘ (ê°œì„ ëœ í•„í„°ë§)"""
        
        source_files = []
        file_patterns = self.config.get('file_patterns', {})
        include_patterns = file_patterns.get('include', ['**/*.java', '**/*.jsp', '**/*.xml'])
        exclude_patterns = file_patterns.get('exclude', ['**/target/**', '**/build/**', '**/.git/**'])
        
        self.logger.debug(f"íŒŒì¼ ìˆ˜ì§‘ ì¤‘: {project_root}")
        self.logger.debug(f"í¬í•¨ íŒ¨í„´: {include_patterns}")
        self.logger.debug(f"ì œì™¸ íŒ¨í„´: {exclude_patterns}")
        
        try:
            for root, dirs, files in os.walk(project_root):
                # ì œì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸°
                dirs[:] = [d for d in dirs if not any(
                    self._match_pattern(os.path.join(root, d), pattern) 
                    for pattern in exclude_patterns
                )]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    
                    # í¬í•¨ íŒ¨í„´ í™•ì¸
                    if any(self._match_pattern(file_path, pattern) for pattern in include_patterns):
                        # ì œì™¸ íŒ¨í„´ í™•ì¸
                        if not any(self._match_pattern(file_path, pattern) for pattern in exclude_patterns):
                            source_files.append(file_path)
                            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {project_root}", exception=e)
            
        self.logger.debug(f"ìˆ˜ì§‘ëœ íŒŒì¼ ìˆ˜: {len(source_files)}")
        return source_files
        
    async def _filter_changed_files(self, source_files: List[str], project_id: int) -> List[str]:
        """ì¦ë¶„ ë¶„ì„ì„ ìœ„í•œ ë³€ê²½ëœ íŒŒì¼ í•„í„°ë§ ë° ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬"""
        changed_files = []
        
        # ê¸°ì¡´ íŒŒì¼ í•´ì‹œ ì •ë³´ ì¡°íšŒ
        session = self.db_manager.get_session()
        try:
            existing_files = session.query(File).filter(File.project_id == project_id).all()
            existing_hashes = {f.path: f.hash for f in existing_files}
            current_files_set = set(source_files)
            existing_files_set = set(existing_hashes.keys())
            
            # ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ DBì—ëŠ” ìˆì§€ë§Œ í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œì—ëŠ” ì—†ëŠ” íŒŒì¼ë“¤)
            deleted_files = existing_files_set - current_files_set
            if deleted_files:
                self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ê°ì§€: {len(deleted_files)}ê°œ")
                await self._handle_deleted_files(deleted_files, project_id)
            
            # ë³€ê²½ëœ íŒŒì¼ ë˜ëŠ” ìƒˆë¡œìš´ íŒŒì¼ ê°ì§€
            for file_path in source_files:
                try:
                    # íŒŒì¼ í¬ê¸° í™•ì¸ìœ¼ë¡œ ì²˜ë¦¬ ì „ëµ ê²°ì •
                    file_size = os.path.getsize(file_path)
                    size_threshold_hash = self.config.get('processing', {}).get('size_threshold_hash', 50 * 1024)  # 50KB
                    size_threshold_mtime = self.config.get('processing', {}).get('size_threshold_mtime', 10 * 1024 * 1024)  # 10MB
                    
                    if file_size < size_threshold_hash:
                        # ì‘ì€ íŒŒì¼: í•­ìƒ í•´ì‹œ ê³„ì‚°
                        with open(file_path, 'rb') as f:
                            content = f.read()
                            current_hash = hashlib.sha256(content).hexdigest()
                        
                        if file_path not in existing_hashes or existing_hashes[file_path] != current_hash:
                            changed_files.append(file_path)
                            self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ì†Œí˜•): {file_path}")
                            
                    elif file_size < size_threshold_mtime:
                        # ì¤‘ê°„ í¬ê¸° íŒŒì¼: mtime ë¨¼ì € í™•ì¸ í›„ í•„ìš”ì‹œ í•´ì‹œ
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        # ê¸°ì¡´ íŒŒì¼ì˜ mtimeê³¼ ë¹„êµ
                        existing_file = next((f for f in existing_files if f.path == file_path), None)
                        if not existing_file or existing_file.mtime < file_mtime:
                            # mtimeì´ ë‹¤ë¥´ë©´ í•´ì‹œ í™•ì¸
                            with open(file_path, 'rb') as f:
                                content = f.read()
                                current_hash = hashlib.sha256(content).hexdigest()
                            
                            if file_path not in existing_hashes or existing_hashes[file_path] != current_hash:
                                changed_files.append(file_path)
                                self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ì¤‘í˜•): {file_path}")
                        
                    else:
                        # ëŒ€í˜• íŒŒì¼: mtimeë§Œ í™•ì¸
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        existing_file = next((f for f in existing_files if f.path == file_path), None)
                        
                        if not existing_file or existing_file.mtime < file_mtime:
                            changed_files.append(file_path)
                            self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ëŒ€í˜•): {file_path}")
                    
                except Exception as e:
                    # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ í¬í•¨
                    self.logger.warning(f"íŒŒì¼ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨ {file_path}: {e}")
                    changed_files.append(file_path)
            
        except Exception as e:
            self.logger.error("ì¦ë¶„ ë¶„ì„ íŒŒì¼ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜", exception=e)
            # ì˜¤ë¥˜ ì‹œ ì „ì²´ íŒŒì¼ ë°˜í™˜
            return source_files
        finally:
            session.close()
            
        return changed_files
    
    async def _handle_deleted_files(self, deleted_files: set, project_id: int):
        """ì‚­ì œëœ íŒŒì¼ë“¤ì˜ ë©”íƒ€ë°ì´í„° ì •ë¦¬"""
        
        self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì‹œì‘: {len(deleted_files)}ê°œ íŒŒì¼")
        
        try:
            # ë©”íƒ€ë°ì´í„° ì—”ì§„ì„ í†µí•´ ì‚­ì œ ì²˜ë¦¬
            cleaned_count = await self.metadata_engine.cleanup_deleted_files(deleted_files, project_id)
            
            # JSP/MyBatis íŒŒì„œì˜ ìºì‹œë„ ì •ë¦¬ (ìˆëŠ” ê²½ìš°)
            if 'jsp_mybatis' in self.parsers:
                for file_path in deleted_files:
                    try:
                        self.parsers['jsp_mybatis'].handle_deleted_file(file_path)
                    except Exception as e:
                        self.logger.warning(f"íŒŒì„œ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            
            self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ ì •ë¦¬ë¨")
            
        except Exception as e:
            self.logger.error("ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜", exception=e)
            raise
        
    def _match_pattern(self, file_path: str, pattern: str) -> bool:
        """íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ (ê°œì„ ëœ ë¡œì§)"""
        import fnmatch
        
        # ì ˆëŒ€ ê²½ë¡œì™€ ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ í™•ì¸
        abs_path = os.path.abspath(file_path)
        rel_path = os.path.relpath(file_path)
        basename = os.path.basename(file_path)
        
        return (fnmatch.fnmatch(abs_path, pattern) or 
                fnmatch.fnmatch(rel_path, pattern) or 
                fnmatch.fnmatch(basename, pattern))
        
    async def _generate_analysis_summary(self, project_id: int, 
                                       analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± (ê°œì„ ëœ í†µê³„)"""
        
        try:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì—”ì§„ ìš”ì•½
            engine_summary = await self.metadata_engine.generate_project_summary(project_id)
            
            # ì¶”ê°€ í†µê³„ ì •ë³´
            enhanced_summary = {
                **engine_summary,
                'processing_stats': {
                    'success_rate': (analysis_results['success_count'] / 
                                   max(1, analysis_results['total_files'])) * 100,
                    'error_rate': (analysis_results['error_count'] / 
                                 max(1, analysis_results['total_files'])) * 100,
                    'files_per_type': {
                        'java': analysis_results['java_files'],
                        'jsp': analysis_results['jsp_files'],
                        'xml': analysis_results['xml_files']
                    }
                },
                'quality_indicators': {
                    'avg_classes_per_java_file': (
                        analysis_results['total_classes'] / max(1, analysis_results['java_files'])
                    ),
                    'avg_methods_per_class': (
                        analysis_results['total_methods'] / max(1, analysis_results['total_classes'])
                    ),
                    'sql_units_found': analysis_results['total_sql_units']
                }
            }
            
            return enhanced_summary
            
        except Exception as e:
            self.logger.error("ë¶„ì„ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜", exception=e)
            return {
                'error': str(e),
                'basic_stats': {'files': 0, 'classes': 0, 'methods': 0, 'sql_units': 0},
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
            
    async def _analyze_files(self, source_files: List[str], project_id: int) -> Dict[str, Any]:
        """ì†ŒìŠ¤ íŒŒì¼ë“¤ ë¶„ì„"""
        
        analysis_results = {
            'total_files': len(source_files),
            'success_count': 0,
            'error_count': 0,
            'java_files': 0,
            'jsp_files': 0,
            'xml_files': 0,
            'total_classes': 0,
            'total_methods': 0,
            'total_sql_units': 0,
            'errors': []
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´
        max_workers = self.config['processing'].get('max_workers', 4)
        semaphore = asyncio.Semaphore(max_workers)
        
        # íŒŒì¼ë³„ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            self._analyze_single_file(file_path, project_id, semaphore) 
            for file_path in source_files
        ]
        
        # ë³‘ë ¬ ì‹¤í–‰
        file_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì§‘ê³„
        for i, result in enumerate(file_results):
            if isinstance(result, Exception):
                analysis_results['errors'].append({
                    'file': source_files[i],
                    'error': str(result)
                })
                analysis_results['error_count'] += 1
            else:
                analysis_results['success_count'] += 1
                
                # íŒŒì¼ íƒ€ì…ë³„ ì¹´ìš´íŒ…
                file_path = source_files[i]
                if file_path.endswith('.java'):
                    analysis_results['java_files'] += 1
                elif file_path.endswith('.jsp'):
                    analysis_results['jsp_files'] += 1
                elif file_path.endswith('.xml'):
                    analysis_results['xml_files'] += 1
                    
                # ìš”ì†Œë³„ ì¹´ìš´íŒ…
                if result:
                    analysis_results['total_classes'] += result.get('classes', 0)
                    analysis_results['total_methods'] += result.get('methods', 0)
                    analysis_results['total_sql_units'] += result.get('sql_units', 0)
                    
        return analysis_results
        
    async def _analyze_single_file(self, file_path: str, project_id: int, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        
        async with semaphore:
            try:
                self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì‹œì‘: {file_path}")
                
                result = {'classes': 0, 'methods': 0, 'sql_units': 0}
                
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ íŒŒì„œ ì„ íƒ
                if 'java' in self.parsers and self.parsers['java'].can_parse(file_path):
                    file_obj, classes, methods, edges = self.parsers['java'].parse_file(file_path, project_id)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    await self.metadata_engine.save_java_analysis(file_obj, classes, methods, edges)
                    
                    result['classes'] = len(classes)
                    result['methods'] = len(methods)
                    
                elif 'jsp_mybatis' in self.parsers and self.parsers['jsp_mybatis'].can_parse(file_path):
                    file_obj, sql_units, joins, filters, edges, vulnerabilities = self.parsers['jsp_mybatis'].parse_file(file_path, project_id)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    await self.metadata_engine.save_jsp_mybatis_analysis(file_obj, sql_units, joins, filters, edges, vulnerabilities)
                    
                    result['sql_units'] = len(sql_units)
                    
                return result
                
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                raise
    
    def _validate_confidence_formula_on_startup(self):
        """
        ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ confidence formula ê²€ì¦ ìˆ˜í–‰
        ì •ì œì‹ ë¶„ì„ #22ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê¸°ëŠ¥
        """
        try:
            self.logger.info("Confidence formula ê²€ì¦ ì‹œì‘...")
            
            # Ground truth ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            if len(self.confidence_validator.ground_truth_data) == 0:
                self.logger.warning("Ground truth ë°ì´í„°ê°€ ì—†ìŒ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                return
            
            # Confidence formula ê²€ì¦ ì‹¤í–‰
            validation_result = self.confidence_validator.validate_confidence_formula()
            
            if 'mean_absolute_error' in validation_result:
                mae = validation_result['mean_absolute_error']
                self.logger.info(f"Confidence formula MAE: {mae:.3f}")
                
                # ì¡°ì • í•„ìš”ì„± í™•ì¸
                if mae > 0.10:  # 10% ì´ìƒ ì˜¤ë¥˜
                    self.logger.warning(f"Confidence ì˜¤ë¥˜ê°€ í¼: {mae:.1%} - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê· ì¸")
                    self._attempt_confidence_calibration()
                else:
                    self.logger.info("í˜„ì¬ confidence formulaê°€ ì ì ˆí•¨")
            
        except Exception as e:
            self.logger.error(f"Confidence ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹œìŠ¤í…œ ì‹œì‘ì„ ë§‰ì§€ëŠ” ì•Šë„ë¡ í•¨
    
    def _attempt_confidence_calibration(self):
        """
        Confidence formula ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„
        """
        try:
            self.logger.info("ìë™ confidence ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„...")
            
            calibration_result = self.confidence_validator.calibrate_weights()
            
            if calibration_result.get('recommendation') == 'apply':
                improvement = calibration_result.get('improvement_percentage', 0)
                self.logger.info(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ {improvement:.1f}% ê°œì„  ì˜ˆìƒ")
                
                # ìë™ìœ¼ë¡œ ì ìš©
                if self.confidence_calibrator.apply_calibration(self.confidence_calculator):
                    self.logger.info("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ì ìš©ë¨")
                else:
                    self.logger.warning("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨")
            else:
                self.logger.info("í˜„ì¬ ê°€ì¤‘ì¹˜ê°€ ìµœì  ìƒíƒœ")
                
        except Exception as e:
            self.logger.error(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def generate_confidence_accuracy_report(self, output_path: str = None) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            output_path: ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì •í™•ë„ ë³´ê³ ì„œ ë°ì´í„°
        """
        try:
            self.logger.info("Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
            
            # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.confidence_validator.generate_confidence_accuracy_report()
            
            # íŒŒì¼ë¡œ ì €ì¥ (ì§€ì •ëœ ê²½ìš°)
            if output_path:
                import json
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                self.logger.info(f"Confidence ë³´ê³ ì„œ ì €ì¥ë¨: {output_path}")
            
            # ìš”ì•½ ì •ë³´ ë¡œê¹…
            if 'validation_report' in report:
                val_report = report['validation_report']
                if 'mean_absolute_error' in val_report:
                    mae = val_report['mean_absolute_error']
                    self.logger.info(f"Current confidence MAE: {mae:.3f}")
                    
                if 'error_distribution' in val_report:
                    error_dist = val_report['error_distribution']
                    excellent_pct = error_dist['excellent']['percentage'] * 100
                    poor_pct = error_dist['poor']['percentage'] * 100
                    self.logger.info(f"Error distribution: {excellent_pct:.1f}% excellent, {poor_pct:.1f}% poor")
            
            return report
            
        except Exception as e:
            self.logger.error(f"Confidence ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def add_ground_truth_entry(self, file_path: str, parser_type: str, 
                               expected_confidence: float, **kwargs) -> bool:
        """
        Ground truth ë°ì´í„° ì—”íŠ¸ë¦¬ ì¶”ê°€
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            parser_type: íŒŒì„œ íƒ€ì…
            expected_confidence: ê¸°ëŒ€ ì‹ ë¢°ë„ (0.0-1.0)
            **kwargs: ê¸°íƒ€ ground truth ë°ì´í„°
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            entry = GroundTruthEntry(
                file_path=file_path,
                parser_type=parser_type,
                expected_confidence=expected_confidence,
                expected_classes=kwargs.get('expected_classes', 0),
                expected_methods=kwargs.get('expected_methods', 0),
                expected_sql_units=kwargs.get('expected_sql_units', 0),
                verified_tables=kwargs.get('verified_tables', []),
                complexity_factors=kwargs.get('complexity_factors', {}),
                notes=kwargs.get('notes', ''),
                verifier=kwargs.get('verifier', 'manual')
            )
            
            self.confidence_validator.add_ground_truth_entry(entry)
            self.logger.info(f"Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ë¨: {file_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(
        description='ì†ŒìŠ¤ ì½”ë“œ ë¶„ì„ ë„êµ¬ - 1ë‹¨ê³„ ë©”íƒ€ì •ë³´ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ ì‚¬ìš©ë²•:
  python main.py PROJECT/sample-app
  python main.py PROJECT/sample-app --project-name "ìƒ˜í”Œ ì•±"
  python main.py PROJECT/sample-app --config custom_config.yaml
        """
    )
    
    parser.add_argument('project_path', help='ë¶„ì„í•  í”„ë¡œì íŠ¸ ê²½ë¡œ')
    parser.add_argument('--config', default='./config/config.yaml', 
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ./config/config.yaml)')
    parser.add_argument('--project-name', help='í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: í´ë”ëª…)')
    parser.add_argument('--incremental', action='store_true', 
                       help='ì¦ë¶„ ë¶„ì„ ëª¨ë“œ (ë³€ê²½ëœ íŒŒì¼ë§Œ ë¶„ì„)')
    parser.add_argument('--verbose', '-v', action='store_true', 
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    parser.add_argument('--quiet', '-q', action='store_true', 
                       help='ìµœì†Œ ë¡œê·¸ ì¶œë ¥')
    
    # Confidence validation ê´€ë ¨ ì˜µì…˜
    parser.add_argument('--validate-confidence', action='store_true',
                       help='Confidence formula ê²€ì¦ ì‹¤í–‰')
    parser.add_argument('--calibrate-confidence', action='store_true', 
                       help='Confidence formula ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜')
    parser.add_argument('--confidence-report', metavar='OUTPUT_FILE',
                       help='Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± (íŒŒì¼ ê²½ë¡œ ì§€ì •)')
    parser.add_argument('--add-ground-truth', nargs=3, 
                       metavar=('FILE_PATH', 'PARSER_TYPE', 'EXPECTED_CONFIDENCE'),
                       help='Ground truth ë°ì´í„° ì—”íŠ¸ë¦¬ ì¶”ê°€')
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²€ì¦
    if not os.path.exists(args.project_path):
        print(f"âŒ ì˜¤ë¥˜: í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.project_path}")
        sys.exit(1)
        
    if not os.path.exists(args.config):
        print(f"âŒ ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.config}")
        sys.exit(1)
        
    try:
        # ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        analyzer = SourceAnalyzer(args.config)
        
        # ë¡œê·¸ ë ˆë²¨ ì¡°ì • (ë¡œê±°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¡œê¹… ì‚¬ìš©)
        if hasattr(analyzer.logger, 'logger'):
            if args.verbose:
                analyzer.logger.logger.setLevel('DEBUG')
            elif args.quiet:
                analyzer.logger.logger.setLevel('WARNING')
        
        # Validation ê´€ë ¨ ëª…ë ¹ì–´ ì²˜ë¦¬
        validation_performed = False
        
        # Ground truth ë°ì´í„° ì¶”ê°€
        if args.add_ground_truth:
            file_path, parser_type, expected_conf = args.add_ground_truth
            try:
                expected_confidence = float(expected_conf)
                if analyzer.add_ground_truth_entry(file_path, parser_type, expected_confidence):
                    print(f"âœ… Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ë¨: {file_path}")
                else:
                    print(f"âŒ Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {file_path}")
                validation_performed = True
            except ValueError:
                print(f"âŒ ì˜¤ë¥˜: ì˜ëª»ëœ confidence ê°’: {expected_conf}")
        
        # Confidence ê²€ì¦
        if args.validate_confidence:
            print("ğŸ” Confidence formula ê²€ì¦ ì‹¤í–‰ ì¤‘...")
            validation_result = analyzer.confidence_validator.validate_confidence_formula()
            
            if 'mean_absolute_error' in validation_result:
                mae = validation_result['mean_absolute_error']
                print(f"ğŸ“Š Confidence MAE: {mae:.3f}")
                
                if 'error_distribution' in validation_result:
                    error_dist = validation_result['error_distribution']
                    excellent_pct = error_dist['excellent']['percentage'] * 100
                    poor_pct = error_dist['poor']['percentage'] * 100
                    print(f"ğŸ“ˆ ì˜¤ë¥˜ ë¶„í¬: {excellent_pct:.1f}% ìš°ìˆ˜, {poor_pct:.1f}% ë¶ˆëŸ‰")
            validation_performed = True
        
        # Confidence ìº˜ë¦¬ë¸Œë ˆì´ì…˜
        if args.calibrate_confidence:
            print("ğŸ”§ Confidence formula ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
            calibration_result = analyzer.confidence_validator.calibrate_weights()
            
            if calibration_result.get('recommendation') == 'apply':
                improvement = calibration_result.get('improvement_percentage', 0)
                print(f"ğŸš€ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ {improvement:.1f}% ê°œì„  ê°€ëŠ¥")
                
                if analyzer.confidence_calibrator.apply_calibration(analyzer.confidence_calculator):
                    print("âœ… ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì™„ë£Œ")
                else:
                    print("âŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨")
            else:
                print("âœ¨ í˜„ì¬ ê°€ì¤‘ì¹˜ê°€ ì´ë¯¸ ìµœì  ìƒíƒœ")
            validation_performed = True
        
        # Confidence ë¦¬í¬íŠ¸ ìƒì„±
        if args.confidence_report:
            print(f"ğŸ“„ Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± ì¤‘: {args.confidence_report}")
            report = analyzer.generate_confidence_accuracy_report(args.confidence_report)
            
            if 'error' not in report:
                print(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {args.confidence_report}")
                
                # ìš”ì•½ ì •ë³´ ì¶œë ¥
                if 'validation_report' in report and 'mean_absolute_error' in report['validation_report']:
                    mae = report['validation_report']['mean_absolute_error']
                    print(f"ğŸ“Š í˜„ì¬ ì˜¤ë¥˜ìœ¨: {mae:.1%}")
            else:
                print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {report['error']}")
            validation_performed = True
        
        # Validation ëª…ë ¹ë§Œ ì‹¤í–‰í•œ ê²½ìš° ì—¬ê¸°ì„œ ì¢…ë£Œ
        if validation_performed:
            print("\nğŸ¯ Validation ì‘ì—… ì™„ë£Œ")
            return
        
        # í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤í–‰
        print(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì‹œì‘: {args.project_path}")
        result = asyncio.run(analyzer.analyze_project(
            args.project_path, 
            args.project_name,
            args.incremental
        ))
        
        # ê²°ê³¼ ì¶œë ¥
        print_analysis_results(result)
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)


def print_analysis_results(result: Dict[str, Any]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    
    print("\n" + "=" * 70)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"í”„ë¡œì íŠ¸: {result['project_name']}")
    print(f"ê²½ë¡œ: {result['project_root']}")
    print(f"ë¶„ì„ ì‹œê°„: {result['analysis_time']:.2f}ì´ˆ")
    print()
    
    # íŒŒì¼ ì²˜ë¦¬ í˜„í™©
    results = result['analysis_results']
    print("íŒŒì¼ ì²˜ë¦¬ í˜„í™©:")
    print(f"  - ì „ì²´ íŒŒì¼: {result['files_analyzed']}ê°œ")
    print(f"  - ì„±ê³µ: {results['success_count']}ê°œ")
    print(f"  - ì‹¤íŒ¨: {results['error_count']}ê°œ")
    if results['error_count'] > 0:
        print(f"  - ì„±ê³µë¥ : {(results['success_count']/results['total_files']*100):.1f}%")
    print()
    
    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„
    print("íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„:")
    print(f"  - Java íŒŒì¼: {results['java_files']}ê°œ")
    print(f"  - JSP íŒŒì¼: {results['jsp_files']}ê°œ")
    print(f"  - XML íŒŒì¼: {results['xml_files']}ê°œ")
    print()
    
    # ë¶„ì„ ê²°ê³¼
    print("ë¶„ì„ ê²°ê³¼:")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {results['total_classes']}ê°œ")
    print(f"  - ë©”ì„œë“œ ìˆ˜: {results['total_methods']}ê°œ")
    print(f"  - SQL êµ¬ë¬¸ ìˆ˜: {results['total_sql_units']}ê°œ")
    print()
    
    # í’ˆì§ˆ ì§€í‘œ
    if 'quality_indicators' in result['summary']:
        quality = result['summary']['quality_indicators']
        print("í’ˆì§ˆ ì§€í‘œ:")
        print(f"  - Java íŒŒì¼ë‹¹ í‰ê·  í´ë˜ìŠ¤ ìˆ˜: {quality['avg_classes_per_java_file']:.1f}ê°œ")
        print(f"  - í´ë˜ìŠ¤ë‹¹ í‰ê·  ë©”ì„œë“œ ìˆ˜: {quality['avg_methods_per_class']:.1f}ê°œ")
        print(f"  - ë°œê²¬ëœ SQL êµ¬ë¬¸: {quality['sql_units_found']}ê°œ")
        print()
    
    # ì˜¤ë¥˜ ì •ë³´ (ìˆëŠ” ê²½ìš°)
    if results['errors']:
        print("ì˜¤ë¥˜ ìƒì„¸:")
        for i, error in enumerate(results['errors'][:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            if isinstance(error, dict) and 'file' in error:
                print(f"  {i+1}. {error['file']}: {error['error']}")
            else:
                print(f"  {i+1}. {error}")
        
        if len(results['errors']) > 5:
            print(f"  ... ë° {len(results['errors'])-5}ê°œ ì¶”ê°€ ì˜¤ë¥˜")
        print()
    
    # ì„±ëŠ¥ ìš”ì•½
    if result['files_analyzed'] > 0 and result['analysis_time'] > 0:
        files_per_sec = result['files_analyzed'] / result['analysis_time']
        print(f"ì„±ëŠ¥: {files_per_sec:.1f}íŒŒì¼/ì´ˆ")
    
    print("=" * 70)


if __name__ == "__main__":
    main()