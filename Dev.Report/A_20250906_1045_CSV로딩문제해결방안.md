# A_20250906_1045_CSV로딩문제해결방안

## 답변

제시해주신 문제 분석은 매우 훌륭하며, 로그에서 확인된 "DB 스키마 CSV 파일을 찾을 수 없습니다"라는 오류 메시지는 경로 문제가 핵심 원인임을 명확히 시사합니다. 제안해주신 해결 방안들과 질문들에 대해 상세히 답변드리겠습니다.

### 1. 로깅 강화 (우선순위 1)

**Q1: `CsvLoader`에 어떤 레벨의 로깅을 추가해야 하나요? (DEBUG, INFO, WARNING, ERROR)**

*   **DEBUG**: 개발 및 디버깅 단계에서 가장 상세한 정보를 기록합니다. 파일 경로 구성 과정, 각 단계별 내부 상태 변화, 작은 단위의 데이터 처리 결과 등을 로깅하여 문제 발생 시 정확한 원인을 추적하는 데 활용합니다.
*   **INFO**: 정상적인 프로그램 흐름을 나타내는 정보를 기록합니다. 파일 로드 시작/완료, 총 로드된 레코드 수, DB 저장 결과 요약 등 사용자가 프로그램의 진행 상황을 파악하는 데 도움이 되는 정보를 로깅합니다.
*   **WARNING**: 예상치 못한 상황이 발생했지만 프로그램의 동작에 치명적이지 않은 경우를 기록합니다. 예를 들어, 선택적 컬럼이 누락되었거나, 일부 레코드에서 경미한 데이터 형식이 맞지 않는 경우 등에 사용합니다.
*   **ERROR**: 복구 불가능하거나 프로그램의 정상적인 동작을 방해하는 심각한 오류를 기록합니다. 파일 없음, DB 연결 실패, CSV 파싱 실패, 필수 컬럼 누락 등으로 인해 해당 파일 또는 전체 프로세스를 더 이상 진행할 수 없을 때 사용합니다.

**Q2: 로깅할 구체적인 정보는 무엇인가요? (파일 경로, 파싱 결과, 저장 결과 등)**

*   **파일 경로**: `CsvLoader`가 시도하는 CSV 파일의 **최종 절대 경로**를 반드시 로깅해야 합니다. (DEBUG/INFO)
*   **파일 존재 여부 및 접근 권한**: 해당 경로에 파일이 실제로 존재하는지, 읽기 권한은 있는지 확인하여 로깅합니다. (DEBUG/INFO)
*   **CSV 파싱 전/후 데이터 샘플**: 파싱을 시도하는 CSV 파일의 첫 몇 줄(예: 5줄)과 헤더 정보를 로깅하여 파일 형식 문제를 진단하는 데 활용합니다. (DEBUG)
*   **각 레코드의 DB 저장 성공/실패 여부**: 특히 DB 저장에 실패한 레코드의 경우, 해당 레코드의 내용과 실패 원인(예: 제약 조건 위반)을 상세히 로깅합니다. (DEBUG/ERROR)
*   **총 로드된 레코드 수, 실패한 레코드 수**: 각 CSV 파일별로 성공적으로 로드된 레코드 수와 실패한 레코드 수를 요약하여 로깅합니다. (INFO)
*   **예외 발생 시 스택 트레이스**: `try-except` 블록에서 예외 발생 시 `logging.exception()`을 사용하여 스택 트레이스를 포함한 상세 오류 정보를 기록합니다. (ERROR)

### 2. 경로 동적 구성 개선 (우선순위 2)

**Q3: 프로젝트 루트 경로를 어떻게 동적으로 계산해야 하나요?**

Python의 `os.path` 모듈을 활용하여 스크립트가 실행되는 환경에 독립적으로 프로젝트 루트 경로를 계산할 수 있습니다. 가장 일반적이고 안정적인 방법은 다음과 같습니다.

```python
import os

def get_project_root():
    # 현재 스크립트 파일의 절대 경로
    current_script_path = os.path.abspath(__file__)
    # 현재 스크립트 파일이 있는 디렉토리
    current_dir = os.path.dirname(current_script_path)

    # 프로젝트 루트를 찾기 위한 마커 파일/폴더 (예: .git, pyproject.toml, README.md)
    # 여기서는 .git 폴더를 마커로 가정합니다.
    marker = '.git'
    
    # 현재 디렉토리부터 상위로 이동하며 마커를 찾습니다.
    while True:
        if marker in os.listdir(current_dir):
            return current_dir
        parent_dir = os.path.dirname(current_dir)
        if parent_dir == current_dir: # 더 이상 상위 디렉토리가 없는 경우 (루트 도달)
            raise Exception(f"프로젝트 루트를 찾을 수 없습니다. '{marker}' 마커가 없습니다.")
        current_dir = parent_dir

# 사용 예시:
# PROJECT_ROOT = get_project_root()
# CSV_BASE_PATH = os.path.join(PROJECT_ROOT, 'project', 'sampleSrc', 'db_schema')
```
이 방식은 스크립트가 어디서 실행되든 `.git` 폴더(또는 다른 마커)를 기준으로 프로젝트의 최상위 경로를 찾아주므로 안정적입니다.

**Q4: 상대경로를 절대경로로 변환하는 로직이 필요한가요?**

**네, 필수적입니다.** `os.getcwd()`에 의존하는 상대경로는 스크립트가 실행되는 현재 작업 디렉토리에 따라 달라지므로 매우 불안정합니다. 항상 `get_project_root()`와 같이 명확하게 정의된 기준 경로(예: 프로젝트 루트)를 정하고, 그 기준으로부터 `os.path.join()`과 `os.path.abspath()`를 조합하여 절대 경로를 구성하는 것이 안정적입니다.

**Q5: Windows와 Linux 환경 모두에서 동작하도록 해야 하나요?**

**네, 필수적입니다.** `os.path` 모듈은 운영체제에 독립적인 경로 처리를 제공합니다. `os.path.join()` 함수는 자동으로 해당 OS의 경로 구분자(Windows의 `\` 또는 Linux의 `/`)를 사용하여 경로를 올바르게 구성해주므로, 이를 적극 활용해야 합니다. 하드코딩된 경로 구분자(예: `path + '/' + filename`)는 사용하지 않아야 합니다.

### 3. 예외 처리 개선 (우선순위 3)

**Q6: CSV 로딩 실패 시 전체 분석을 중단해야 하나요, 아니면 계속 진행해야 하나요?**

**부분 실패 시 계속 진행**하는 것을 강력히 권장합니다. 하나의 CSV 파일 로딩 실패가 전체 분석을 중단시키는 것은 비효율적입니다. 대신, 실패한 파일에 대한 명확한 로깅과 함께 다음 파일로 진행해야 합니다. 단, `ALL_TABLES.csv`와 같이 시스템의 핵심 스키마 정의를 담고 있어 없으면 다른 모든 분석이 무의미해지는 **필수적인 핵심 CSV 파일** 로딩 실패 시에는 전체 분석 중단을 고려할 수 있습니다. 이는 시스템의 핵심 기능에 따라 유연하게 결정되어야 합니다.

**Q7: 부분적으로 성공한 CSV 파일들은 어떻게 처리해야 하나요?**

성공적으로 로드된 데이터는 그대로 데이터베이스에 유지하고, 실패한 부분(예: 특정 레코드 파싱 오류)에 대해서만 오류를 기록하고 해당 레코드를 건너뛰어야 합니다. 가능하다면, 실패한 레코드의 원본 내용과 실패 원인을 별도의 로그 파일이나 오류 리포트에 기록하여 수동 검토 및 재처리할 수 있도록 하는 것이 좋습니다.

**Q8: 사용자에게 어떤 형태로 오류를 알려야 하나요?**

*   **콘솔 출력**: 스크립트 실행 중 사용자에게 즉각적인 피드백을 제공하기 위해 간결하고 명확한 오류 메시지를 콘솔에 출력합니다. (예: "ERROR: CSV 파일 로드 실패 - [파일명]: [오류 요약]")
*   **로그 파일**: 상세한 오류 정보(스택 트레이스, 실패한 데이터 샘플, 발생 시간 등)를 로그 파일에 기록하여 사후 분석 및 디버깅에 활용합니다.
*   **리포트 파일**: 분석 완료 후, 전체 CSV 로딩 성공/실패 요약, 각 파일별 로드된 레코드 수, 주요 오류 목록 등을 포함하는 요약 리포트 파일을 생성하여 사용자에게 제공합니다.

### 4. CSV 파일 검증 로직 추가 (우선순위 4)

**Q9: CSV 파일의 필수 컬럼은 무엇인가요?**

`CsvLoader`가 각 CSV 파일(예: `ALL_TABLES.csv`, `PK_INFO.csv`)에서 기대하는 **필수 컬럼 목록을 명시적으로 정의**해야 합니다. 이는 코드 내부에 상수 형태로 정의하거나, 외부 설정 파일(예: YAML, JSON)에서 읽어올 수 있습니다. 로드 시 CSV 파일의 헤더가 이 필수 컬럼 목록을 모두 포함하는지 검증해야 합니다.

**Q10: CSV 파일 형식이 잘못된 경우 어떻게 처리해야 하나요?**

*   **헤더 검증**: `pandas.read_csv`로 파일을 읽은 후, 데이터프레임의 컬럼명(`df.columns`)이 예상하는 필수 컬럼을 모두 포함하는지 확인합니다. 헤더가 예상과 다르면 오류 로깅 후 해당 파일 로드를 건너뜁니다.
*   **데이터 타입 검증**: 특정 컬럼이 숫자여야 하는데 문자열이거나, 날짜 형식이어야 하는데 잘못된 형식인 경우 등을 검증합니다. `pandas`의 `astype()` 또는 `to_numeric()`, `to_datetime()` 등을 사용하고 `errors='coerce'` 옵션을 활용하여 변환 실패 시 `NaN` 등으로 처리한 후, `NaN` 값을 포함하는 레코드를 찾아 로깅하고 건너뛸 수 있습니다.
*   **오류 레코드 격리**: 잘못된 형식의 레코드는 별도의 CSV 파일에 기록하여 추후 수동 검토 및 수정할 수 있도록 합니다.

**Q11: 인코딩 문제는 어떻게 해결해야 하나요?**

`pandas.read_csv` 사용 시 `encoding` 파라미터를 활용합니다.

*   **일반적인 인코딩 순차 시도**: 가장 흔히 사용되는 인코딩(`utf-8`, `cp949` (Windows 한글), `euc-kr`)을 순서대로 시도하는 로직을 구현합니다.
    ```python
    encodings_to_try = ['utf-8', 'cp949', 'euc-kr']
    df = None
    for enc in encodings_to_try:
        try:
            df = pd.read_csv(file_path, encoding=enc)
            break # 성공하면 루프 종료
        except UnicodeDecodeError:
            continue # 다음 인코딩 시도
    if df is None:
        # 모든 인코딩 시도 실패 시 오류 처리
        logging.error(f"CSV 파일 '{file_path}'의 인코딩을 감지할 수 없습니다.")
    ```
*   **자동 감지 라이브러리 활용**: `chardet`와 같은 라이브러리를 사용하여 파일의 인코딩을 자동 감지하는 고급 방법도 고려할 수 있습니다. 하지만 이는 추가적인 의존성을 발생시킵니다.

### 5. 성능 및 확장성 관련

**Q12: 대용량 CSV 파일 처리를 고려해야 하나요?**

**네, 고려해야 합니다.** `pandas.read_csv`는 기본적으로 대용량 파일 처리에 효율적이지만, 매우 큰 파일(수백 MB 이상)의 경우 메모리 문제가 발생할 수 있습니다.

*   **`chunksize` 활용**: `pandas.read_csv(file_path, chunksize=10000)`와 같이 `chunksize` 파라미터를 사용하여 파일을 청크(덩어리) 단위로 읽어 메모리 사용량을 최적화하고, 각 청크를 처리 후 DB에 삽입하는 방식을 고려합니다.
*   **DB 배치 삽입**: DB 삽입 시 `cursor.executemany()`를 사용하여 여러 레코드를 한 번에 삽입하는 배치 삽입 방식으로 성능을 향상시킵니다.

**Q13: CSV 파일이 여러 개일 때 병렬 처리가 필요한가요?**

파일 수가 많고 각 CSV 파일의 로딩 작업이 독립적이라면 `multiprocessing` 또는 `concurrent.futures` 모듈을 사용하여 병렬 처리를 고려할 수 있습니다.

*   **주의사항**: DB 연결 및 쓰기 작업 시 동시성 문제(락, 데드락)를 주의해야 합니다. 각 프로세스/스레드마다 별도의 DB 연결을 사용하거나, 쓰기 작업을 직렬화하는 전략(예: 모든 데이터를 메모리에 모은 후 단일 프로세스에서 DB에 일괄 삽입)이 필요합니다.

**Q14: 메모리 사용량 최적화가 필요한가요?**

**네, 필요합니다.** 특히 대용량 CSV 파일을 처리하거나 여러 파일을 동시에 처리할 경우 중요합니다.

*   **`chunksize`를 이용한 청크 단위 처리**: 위에서 언급했듯이, 파일을 청크 단위로 읽어 메모리에 한 번에 로드되는 데이터 양을 줄입니다.
*   **`dtype` 파라미터**: `pandas.read_csv`의 `dtype` 파라미터를 사용하여 각 컬럼의 데이터 타입을 명시적으로 지정하여 `pandas`가 불필요하게 큰 데이터 타입을 할당하는 것을 방지하고 메모리 사용량을 감소시킵니다.
*   **불필요한 객체 즉시 해제**: 처리 완료된 데이터프레임이나 큰 객체는 `del` 키워드를 사용하여 명시적으로 메모리에서 해제하고, 가비지 컬렉션을 유도합니다.

---

**구현 우선순위 재확인:**

제안해주신 구현 우선순위는 매우 합리적입니다. 특히 **로깅 강화**를 통해 정확한 원인을 파악하는 것이 가장 중요하며, 이를 통해 **경로 동적 구성 개선**과 **예외 처리 개선**을 효과적으로 진행할 수 있을 것입니다.
