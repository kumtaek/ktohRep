# PRD 개선 제안

이 문서는 `prd_v3.md`에서 제시한 소스 분석·영향평가 시스템을 바탕으로, 최신 연구와 오픈소스 생태계를 조사하여 더 나은 라이브러리·모델·알고리즘을 제안합니다. 각각의 제안은 폐쇄망 환경에서도 구동 가능하도록 오프라인 사용을 전제로 합니다.

## 1. 정적 분석 파서 및 언어 지원 개선

### Tree‑Sitter 기반 다국어 파싱

현재 PRD에서는 **JavaParser**를 기본 Java AST 파서로 사용합니다. Symflower의 경험담에 따르면 JavaParser에서 **Tree‑Sitter**로 전환했을 때 파서 속도가 **36배** 빨라졌고, 부분 수정된 코드에 대해서도 증분 파싱이 가능해졌습니다【765916487391108†L27-L43】. Tree‑Sitter는 언어별 규칙을 정의한 *grammar* 파일만 있으면 **다양한 언어를 빠르게 파싱**할 수 있고, 구문 오류에도 견고하게 동작합니다【765916487391108†L50-L68】. 따라서 Java뿐 아니라 JSP, JavaScript/TypeScript, Python 등 다양한 언어를 **단일 파서 프레임워크**로 처리할 수 있으며, 증분 분석과 에디터 통합까지 용이합니다.

*추천*: JavaParser를 유지하되, 신규 개발은 **Tree‑Sitter** 기반 파서를 병행 도입하여 증분 파싱·다언어 지원을 확보합니다. C 또는 Rust FFI를 활용한 파이썬 바인딩을 사용해 오프라인 환경에 포함할 수 있습니다.

### ANTLR 기반 SQL/PL‒SQL 파서 확장

MyBatis와 JSP 스크립틀릿의 SQL 구문 분석에는 JSQLParser를 사용하지만, Oracle PL/SQL 확장과 동적 SQL 처리는 한계가 있습니다. Bytebase의 SQL 파서 비교에서 **ANTLR** 기반 파서가 **인간이 읽기 쉬운 문법** 파일을 제공하여 **방언별 기능 추가**와 맞춤화에 적합함을 강조합니다【72932677096023†L159-L175】. Oracle dialect용 grammar를 확장하면 Stored Procedure/Function 구문, 동적 SQL 변형, 트리거 등을 정밀하게 파싱할 수 있습니다.

*추천*: PL/SQL 분석을 위해 `antlr-plsql`을 도입하고, 필요시 Oracle SQL 방언을 지원하는 SQLGlot와 병행하여 복잡한 SQL을 정규화합니다. Custom grammar를 통해 `JOIN`/필수 필터 추출 알고리즘의 정확도를 높일 수 있습니다.

### Tree‑Sitter/ANTLR를 활용한 다른 언어 지원

향후 확장 계획에 포함된 Python, JavaScript/TypeScript, Vue.js 분석에는 Tree‑Sitter grammar를 활용하여 클래스/함수/컴포넌트 구조를 추출하고, 타입 정보를 얻기 위해 TypeScript 컴파일러 API나 `pyright`를 통합합니다. 이를 통해 프론트엔드까지 전체 호출 그래프를 연결할 수 있습니다.

## 2. 임베딩과 검색 개선

### 하이브리드 임베딩 + 교차 인코더

PRD는 `BAAI/bge-m3`와 `ko-sentence-transformers` 등 한국어 임베딩을 채택하고 있습니다. BGE‑M3 모델 카드는 **100개 이상의 언어**에서 동작하며 **장문 컨텍스트(최대 8 192 토큰)**를 지원하고, 스파스(BM25)·멀티벡터 검색을 동시에 수행하는 **하이브리드 임베딩**을 권장합니다【689000657501117†L65-L73】. 또한, 초기 검색 후에는 `bge-reranker`와 같은 **교차 인코더 리랭커**를 적용하면 단순 임베딩 검색보다 정확도가 높아집니다【689000657501117†L79-L90】. 2024년 3월 출시된 **BGE reranker v2**는 더 긴 입력과 다국어를 지원하며 ranking 성능이 향상되었습니다【726653985607223†L92-L99】.

*추천*: 검색 단계에서 ① BGE‑M3 임베딩으로 top‑k 후보를 추출하고 ② `BAAI/bge-reranker-base` 또는 v2 모델로 교차 인코더 재랭킹을 수행합니다. 또한, 희소 검색을 병합하기 위해 **BM25 점수**와 임베딩 점수를 혼합하는 **hybrid retrieval** 전략을 적용하면 검색 재현율과 정확도를 동시에 향상할 수 있습니다.

### 고성능 벡터 데이터베이스 검토

현재 PRD는 `FAISS`를 사용하여 벡터 인덱스를 구축합니다. **Faiss**는 메모리 내 대규모 벡터 검색에 적합하지만 지속성이나 필터링 기능이 부족합니다. DataCamp의 벡터 DB 비교에서 **Qdrant**는 *HNSW* 기반 엔진과 **Payload 필터링** 기능을 제공하고, **Rust로 작성되어 수평 확장이 용이**한 점을 강조합니다【323443627363361†L362-L374】. **Milvus**는 **분산 아키텍처**를 통해 **수십억 개의 벡터**를 지원하며 TensorFlow/PyTorch 통합을 제공합니다【323443627363361†L382-L394】. 이러한 DB는 온프레미스 서버에 설치해 멀티테넌트 검색 및 메타 정보 필터링 조건을 처리할 수 있습니다.

*추천*: 초기 단계에서는 FAISS로 시작하되, 규모가 커질 경우 **Qdrant** 또는 **Milvus**를 도입하여 벡터 검색의 확장성과 필터링 기능을 확보합니다. `pgvector`를 활용해 Oracle 전환 시 RDB 내에서 벡터를 저장하는 방안도 평가해 볼 수 있습니다【323443627363361†L405-L417】.

## 3. LLM 모델 개선 및 옵션

### 대안 코드 LLM 검토

PRD에서는 질의응답과 영향평가를 위해 **Qwen 2.5 7B/32B**를 사용합니다. 2024년 6월 공개된 **Qwen 2.5‑Coder** 시리즈는 0.5B–32B 크기로 출시되었으며, **5.5 조 토큰**으로 사전학습되었고 코드 생성·수정·이해 분야의 **10개 이상 벤치마크에서 동급 모델을 상회**하는 성능을 보입니다【871847968524855†L94-L100】. 보고서는 32B 모델이 GPT‑4o와 비슷한 코딩 성능을 갖추고, 다양한 크기를 통해 비용–성능 균형을 조절할 수 있음을 강조합니다【871847968524855†L176-L184】.

또한 **DeepSeek‑Coder** 7B/33B 모델은 *HumanEval* 및 *MBPP* 벤치마크에서 **CodeLlama‑34B**보다 7–11% 더 높은 점수를 기록하며, 33B 인스트럭션 모델은 GPT‑3.5‑Turbo를 능가합니다【846618056811416†L365-L371】. 

*추천*: 제한된 GPU 자원(A30 24 GB)에서도 수행 가능한 **DeepSeek‑Coder‑7B**나 **Qwen 2.5‑Coder‑7B**를 도입해 코드 이해·생성을 위한 LLM으로 선택지를 넓힐 수 있습니다. 보다 정확한 영향평가가 필요할 경우 **32B** 또는 **33B** 모델을 사용하여 폴백 모델로 설정합니다. 

### 안전한 SAST 도구 통합

보안 취약점 분석을 강화하기 위해 **CodeQL**이나 **Semgrep**과 같은 오픈소스 SAST 도구를 오프라인 모드로 실행하는 방안을 검토합니다. 이러한 도구는 SQL Injection, XSS, 경로 조작 등의 취약 패턴을 정적 분석하여 **보안 요약**을 자동 생성해줍니다. 이후 LLM이 수정 제안을 생성할 때 SAST 보고서를 근거로 활용하면 더 정확한 `vulnerability_fixes`를 만들 수 있습니다.

## 4. 그래프 데이터베이스 및 호출 그래프 개선

Neo4j는 강력한 그래프 쿼리와 시각화 기능을 제공하지만 라이선스와 성능 제약이 있습니다. 오픈소스 **NebulaGraph** 또는 **JanusGraph**는 **수평 확장**과 **자바 친화적 API**를 제공하며, Oracle 통합 시 복잡한 호출 그래프를 보다 효율적으로 저장할 수 있습니다. 향후 데이터 규모와 쿼리 복잡도에 따라 Neo4j에서 이러한 대안으로 마이그레이션하는 것을 고려할 수 있습니다.

## 5. 검색 및 오케스트레이션 고도화

### 멀티‑홉 검색과 계획 기반 질의해결

PRD의 오케스트레이터는 기본적으로 LangChain/LangGraph를 이용해 질의 의도 분류, 초기 검색, 멀티‑홉 확장, 교차 인코더 재랭킹 순으로 진행합니다. 이를 보완하기 위해 다음을 제안합니다.

1. **LlamaIndex 통합** – LlamaIndex(구 GPT‑Index)는 문서와 그래프 구조를 추상화하여 **다양한 벡터 스토어**와 **LLM**을 모듈식으로 연결합니다. 오프라인 환경에서도 동작하며, LangChain보다 가벼운 오케스트레이션을 제공할 수 있습니다.
2. **검색 계획 생성 LLM** – Qwen 또는 DeepSeek 모델에게 질의의 의도와 필요한 엔티티를 요약하도록 하고, 이를 기반으로 **검색 계획**을 동적으로 생성합니다. 검색 계획에는 엔티티 식별, 탐색 횟수, 필터 조건 등이 포함되어 멀티‑홉 검색의 효율을 높일 수 있습니다.
3. **결과 캐싱·변경 감지** – 증분 분석과 연계하여, 프로젝트 변화가 없을 경우 이미 저장된 벡터·그래프·요약을 재사용하고, 변경이 있는 부분만 재계산합니다. 이는 A30 단일 서버의 메모리와 시간 자원을 절약합니다.

## 6. 기타 개선 사항

* **실행 환경 패키징**: 모든 파서, 모델, DB 바이너리를 컨테이너화하고 `conda-lock`이나 `pip-tools`를 사용한 **정확한 의존성 버전 고정**을 권장합니다. 이는 오프라인 설치 시 호환성 문제를 예방합니다.
* **로그 및 모니터링**: Prometheus/Grafana 외에도 **OpenTelemetry**를 사용하여 API 호출, LLM latency, 파서 오류 등을 추적하면 병목 원인 분석과 용량 계획에 도움이 됩니다.
* **다국어 UI**: 질의·응답 인터페이스와 문서화를 한국어/영어로 제공하고, RAG 시스템의 프롬프트도 이중 언어를 지원하도록 설계합니다.

---

위 개선 사항들은 시스템의 성능, 재현율, 유지보수성을 향상시키는 데 목적이 있습니다. 특히 Tree‑Sitter 기반 파싱과 하이브리드 검색 전략, 신규 코드 LLM 도입은 정적 분석과 Q&A 정확도를 크게 개선할 수 있습니다. 오프라인 환경과 보수적 포괄 원칙을 준수하며, 추후 요구되는 언어·프레임워크 확장에도 대비합니다.
