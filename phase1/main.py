"""
ì†ŒìŠ¤ ë¶„ì„ê¸° ë©”ì¸ ëª¨ë“ˆ
ë¦¬ë·° ì˜ê²¬ì„ ë°˜ì˜í•œ ê°œì„ ì‚¬í•­ë“¤ì´ ì ìš©ëœ 1ë‹¨ê³„ ë©”íƒ€ì •ë³´ ìƒì„± ì‹œìŠ¤í…œ
"""

import os
import sys
import argparse
import yaml
import asyncio
import hashlib
from pathlib import Path
from typing import Dict, Any, List
import time
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python íŒ¨ìŠ¤ì— ì¶”ê°€ (ìˆ˜ì •ë¨: phase1ì´ ë£¨íŠ¸ê°€ ë¨)
project_root = Path(__file__).parent.parent  # phase1ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
sys.path.insert(0, str(project_root))

from models.database import DatabaseManager, File
from parsers.java_parser import JavaParser
from parsers.jsp_mybatis_parser import JspMybatisParser
from parsers.sql_parser import SqlParser
from database.metadata_engine import MetadataEngine
from utils.csv_loader import CsvLoader
from utils.logger import setup_logging, LoggerFactory, PerformanceLogger
from utils.confidence_calculator import ConfidenceCalculator
from utils.confidence_validator import ConfidenceValidator, ConfidenceCalibrator, GroundTruthEntry


class SourceAnalyzer:
    """ì†ŒìŠ¤ ë¶„ì„ê¸° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str):
        """
        ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.logger = setup_logging(self.config)
        
        self.logger.info("=" * 60)
        self.logger.info("ê°œì„ ëœ ì†ŒìŠ¤ ë¶„ì„ê¸° ì‹œì‘")
        self.logger.info("=" * 60)
        
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.db_manager = DatabaseManager(self.config)
            self.db_manager.initialize()
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            
            # íŒŒì„œë“¤ ì´ˆê¸°í™”
            self.parsers = self._initialize_parsers()
            self.logger.info(f"íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ: {list(self.parsers.keys())}")
            
            # ë©”íƒ€ë°ì´í„° ì—”ì§„ ì´ˆê¸°í™”
            self.metadata_engine = MetadataEngine(self.config, self.db_manager)
            
            # CSV ë¡œë” ì´ˆê¸°í™”
            self.csv_loader = CsvLoader(self.config)
            
            # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
            self.confidence_calculator = ConfidenceCalculator(self.config)
            
            # Confidence Validation ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            self.confidence_validator = ConfidenceValidator(self.config, self.confidence_calculator)
            self.confidence_calibrator = ConfidenceCalibrator(self.confidence_validator)
            
            # ì‹œì‘ ì‹œ ìë™ìœ¼ë¡œ confidence formula ê²€ì¦ ì‹¤í–‰
            self._validate_confidence_formula_on_startup()
            
        except Exception as e:
            self.logger.critical("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨", exception=e)
            raise
            
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ (ê°œì„ ëœ ì˜ˆì™¸ ì²˜ë¦¬)"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                raw = f.read()
            # í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜ ì§€ì› (${VAR})
            config = yaml.safe_load(os.path.expandvars(raw))
                
            # ê¸°ë³¸ê°’ ì„¤ì •
            self._set_default_config(config)
            
            return config
            
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            raise
        except yaml.YAMLError as e:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise
        except Exception as e:
            print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            raise
            
    def _set_default_config(self, config: Dict[str, Any]):
        """ê¸°ë³¸ ì„¤ì •ê°’ ì„¤ì •"""
        
        # ê¸°ë³¸ ì²˜ë¦¬ ì„¤ì •
        if 'processing' not in config:
            config['processing'] = {}
        processing = config['processing']
        processing.setdefault('max_workers', 4)
        processing.setdefault('batch_size', 10)
        processing.setdefault('chunk_size', 512)
        processing.setdefault('confidence_threshold', 0.5)
        
        # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
        if 'logging' not in config:
            config['logging'] = {}
        logging_config = config['logging']
        logging_config.setdefault('level', 'INFO')
        logging_config.setdefault('file', './logs/analyzer.log')
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ ì„¤ì •
        if 'confidence' not in config:
            config['confidence'] = {}
        confidence_config = config['confidence']
        confidence_config.setdefault('weights', {
            'ast': 0.4,
            'static': 0.3,
            'db_match': 0.2,
            'heuristic': 0.1
        })
        
        # LLM ë³´ê°• ê¸°ë³¸ ì„¤ì • (llm ë˜ëŠ” llm_assist ì„¹ì…˜ê³¼ í˜¸í™˜)
        if 'llm_assist' not in config:
            config['llm_assist'] = {}
        llm_cfg = config['llm_assist']
        # llm ì„¹ì…˜ì´ ìˆì„ ê²½ìš° ì¼ë¶€ ê°’ì„ ê¸°ë³¸ì¹˜ë¡œ ì „íŒŒ
        legacy_llm = config.get('llm', {})
        if isinstance(legacy_llm, dict):
            # vLLM endpoint -> base_url
            llm_cfg.setdefault('vllm_base_url', legacy_llm.get('vllm_endpoint'))
            # base_model -> vllm_model ê¸°ë³¸
            llm_cfg.setdefault('vllm_model', legacy_llm.get('base_model'))
            # temperature/max_tokens ì „íŒŒ
            if 'temperature' in legacy_llm:
                llm_cfg.setdefault('temperature', legacy_llm.get('temperature'))
            if 'max_tokens' in legacy_llm:
                llm_cfg.setdefault('max_tokens', legacy_llm.get('max_tokens'))
        llm_cfg.setdefault('enabled', True)
        llm_cfg.setdefault('provider', 'auto')  # auto|ollama|vllm
        llm_cfg.setdefault('low_conf_threshold', 0.6)
        llm_cfg.setdefault('max_calls_per_run', 50)
        llm_cfg.setdefault('file_max_lines', 1200)
        llm_cfg.setdefault('temperature', 0.0)
        llm_cfg.setdefault('max_tokens', 512)
        llm_cfg.setdefault('strict_json', True)
        llm_cfg.setdefault('cache', True)
        llm_cfg.setdefault('cache_dir', './output/llm_cache')
        llm_cfg.setdefault('log_prompt', False)
        llm_cfg.setdefault('dry_run', False)
        llm_cfg.setdefault('fallback_to_ollama', True)
        
    def _initialize_parsers(self) -> Dict[str, Any]:
        """íŒŒì„œë“¤ ì´ˆê¸°í™” (ê°œì„ ëœ ì˜ˆì™¸ ì²˜ë¦¬)"""
        parsers = {}
        
        try:
            # Java íŒŒì„œ
            if self.config.get('parsers', {}).get('java', {}).get('enabled', True):
                parsers['java'] = JavaParser(self.config)
                self.logger.info("Java íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
            # JSP/MyBatis íŒŒì„œ  
            if self.config.get('parsers', {}).get('jsp', {}).get('enabled', True):
                parsers['jsp_mybatis'] = JspMybatisParser(self.config)
                self.logger.info("JSP/MyBatis íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
            # SQL íŒŒì„œ
            if self.config.get('parsers', {}).get('sql', {}).get('enabled', True):
                parsers['sql'] = SqlParser(self.config)
                self.logger.info("SQL íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            self.logger.error("íŒŒì„œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜", exception=e)
            raise
            
        if not parsers:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        return parsers
        
    async def analyze_project(self, project_root: str, project_name: str = None, incremental: bool = False) -> Dict[str, Any]:
        """
        í”„ë¡œì íŠ¸ ì „ì²´ ë¶„ì„ (ê°œì„ ëœ ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            project_root: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„ (ì—†ìœ¼ë©´ í´ë”ëª… ì‚¬ìš©)
            incremental: ì¦ë¶„ ë¶„ì„ ëª¨ë“œ (ë³€ê²½ëœ íŒŒì¼ë§Œ ë¶„ì„)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        if not project_name:
            project_name = os.path.basename(project_root.rstrip('/\\'))
            
        start_time = time.time()
        
        with PerformanceLogger(self.logger, f"í”„ë¡œì íŠ¸ '{project_name}' ì „ì²´ ë¶„ì„"):
            
            try:
                # 1. í”„ë¡œì íŠ¸ ë“±ë¡
                self.logger.info(f"í”„ë¡œì íŠ¸ ë“±ë¡ ì¤‘: {project_name} ({project_root})")
                project_id = await self.metadata_engine.create_project(project_root, project_name)
                
                # 2. DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ
                await self._load_db_schema(project_root, project_name, project_id)
                
                # 3. ì†ŒìŠ¤ íŒŒì¼ë“¤ ìˆ˜ì§‘
                source_files = self._collect_source_files(project_root)
                self.logger.info(f"ë°œê²¬ëœ ì†ŒìŠ¤ íŒŒì¼: {len(source_files)}ê°œ")
                
                # ì¦ë¶„ ë¶„ì„ ëª¨ë“œì¸ ê²½ìš° ë³€ê²½ëœ íŒŒì¼ë§Œ ì„ ë³„
                if incremental:
                    source_files = await self._filter_changed_files(source_files, project_id)
                    self.logger.info(f"ì¦ë¶„ ë¶„ì„ ëŒ€ìƒ íŒŒì¼: {len(source_files)}ê°œ")
                
                if not source_files:
                    self.logger.warning("ë¶„ì„í•  ì†ŒìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return self._create_empty_result(project_id, project_name)
                
                # 4. íŒŒì¼ë“¤ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬)
                analysis_results = await self._analyze_files(source_files, project_id)
                
                # 5. ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
                self.logger.info("ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶• ì¤‘")
                await self.metadata_engine.build_dependency_graph(project_id)
                
                # 6. ë¶„ì„ ê²°ê³¼ ìš”ì•½
                summary = await self._generate_analysis_summary(project_id, analysis_results)
                
                total_time = time.time() - start_time
                self.logger.info(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì™„ë£Œ: {project_name} ({total_time:.2f}ì´ˆ)")
                
                return {
                    'project_id': project_id,
                    'project_name': project_name,
                    'project_root': project_root,
                    'analysis_time': total_time,
                    'files_analyzed': len(source_files),
                    'analysis_results': analysis_results,
                    'summary': summary
                }
                
            except Exception as e:
                self.logger.error(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {project_name}", exception=e)
                raise
                
    def _create_empty_result(self, project_id: int, project_name: str) -> Dict[str, Any]:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        return {
            'project_id': project_id,
            'project_name': project_name,
            'analysis_time': 0,
            'files_analyzed': 0,
            'analysis_results': {
                'total_files': 0,
                'success_count': 0,
                'error_count': 0,
                'java_files': 0,
                'jsp_files': 0,
                'xml_files': 0,
                'total_classes': 0,
                'total_methods': 0,
                'total_sql_units': 0,
                'errors': []
            },
            'summary': {
                'basic_stats': {
                    'files': 0, 'classes': 0, 'methods': 0, 'sql_units': 0,
                    'joins': 0, 'filters': 0
                },
                'language_distribution': {},
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
        }
        
    async def _load_db_schema(self, project_root: str, project_name: str, project_id: int):
        """DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë“œ (ê°œì„ ëœ ì˜¤ë¥˜ ì²˜ë¦¬)"""
        
        # configì—ì„œ path_templateì„ ê°€ì ¸ì™€ project_nameìœ¼ë¡œ í¬ë§·íŒ…
        db_schema_template = self.config['db_schema']['path_template']
        db_schema_path = db_schema_template.format(project_name=project_name)

        if not os.path.exists(db_schema_path):
            self.logger.warning(f"DB ìŠ¤í‚¤ë§ˆ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤: {db_schema_path}")
            return
            
        self.logger.info(f"DB ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¡œë”© ì¤‘: {db_schema_path}")
        
        required_files = self.config['db_schema']['required_files']
        loaded_files = []
        
        for csv_file in required_files:
            csv_path = os.path.join(db_schema_path, csv_file)
            
            try:
                if os.path.exists(csv_path):
                    result = await self.csv_loader.load_csv(csv_path, project_id)
                    loaded_files.append(csv_file)
                    self.logger.info(f"CSV ë¡œë“œ ì™„ë£Œ: {csv_file} ({result['records']}ê±´)")
                else:
                    self.logger.warning(f"CSV íŒŒì¼ ì—†ìŒ: {csv_path}")
                    
            except Exception as e:
                self.logger.error(f"CSV ë¡œë“œ ì‹¤íŒ¨: {csv_file}", exception=e)
                
        if loaded_files:
            self.logger.info(f"DB ìŠ¤í‚¤ë§ˆ ë¡œë”© ì™„ë£Œ: {len(loaded_files)}/{len(required_files)} íŒŒì¼")
        else:
            self.logger.warning("DB ìŠ¤í‚¤ë§ˆ íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
    def _collect_source_files(self, project_root: str) -> List[str]:
        """ì†ŒìŠ¤ íŒŒì¼ë“¤ ìˆ˜ì§‘ (ê°œì„ ëœ í•„í„°ë§)"""
        
        source_files = []
        file_patterns = self.config.get('file_patterns', {})
        include_patterns = file_patterns.get('include', ['**/*.java', '**/*.jsp', '**/*.xml'])
        exclude_patterns = file_patterns.get('exclude', ['**/target/**', '**/build/**', '**/.git/**'])
        
        self.logger.debug(f"íŒŒì¼ ìˆ˜ì§‘ ì¤‘: {project_root}")
        self.logger.debug(f"í¬í•¨ íŒ¨í„´: {include_patterns}")
        self.logger.debug(f"ì œì™¸ íŒ¨í„´: {exclude_patterns}")
        
        try:
            for root, dirs, files in os.walk(project_root):
                # ì œì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸°
                dirs[:] = [d for d in dirs if not any(
                    self._match_pattern(os.path.join(root, d), pattern) 
                    for pattern in exclude_patterns
                )]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    
                    # í¬í•¨ íŒ¨í„´ í™•ì¸
                    if any(self._match_pattern(file_path, pattern) for pattern in include_patterns):
                        # ì œì™¸ íŒ¨í„´ í™•ì¸
                        if not any(self._match_pattern(file_path, pattern) for pattern in exclude_patterns):
                            source_files.append(file_path)
                            
        except Exception as e:
            self.logger.error(f"íŒŒì¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {project_root}", exception=e)
            
        self.logger.debug(f"ìˆ˜ì§‘ëœ íŒŒì¼ ìˆ˜: {len(source_files)}")
        return source_files
        
    async def _filter_changed_files(self, source_files: List[str], project_id: int) -> List[str]:
        """ì¦ë¶„ ë¶„ì„ì„ ìœ„í•œ ë³€ê²½ëœ íŒŒì¼ í•„í„°ë§ ë° ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬"""
        changed_files = []
        
        # ê¸°ì¡´ íŒŒì¼ í•´ì‹œ ì •ë³´ ì¡°íšŒ
        session = self.db_manager.get_session()
        try:
            existing_files = session.query(File).filter(File.project_id == project_id).all()
            existing_hashes = {f.path: f.hash for f in existing_files}
            current_files_set = set(source_files)
            existing_files_set = set(existing_hashes.keys())
            
            # ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ DBì—ëŠ” ìˆì§€ë§Œ í˜„ì¬ íŒŒì¼ ì‹œìŠ¤í…œì—ëŠ” ì—†ëŠ” íŒŒì¼ë“¤)
            deleted_files = existing_files_set - current_files_set
            if deleted_files:
                self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ê°ì§€: {len(deleted_files)}ê°œ")
                await self._handle_deleted_files(deleted_files, project_id)
            
            # ë³€ê²½ëœ íŒŒì¼ ë˜ëŠ” ìƒˆë¡œìš´ íŒŒì¼ ê°ì§€
            for file_path in source_files:
                try:
                    # íŒŒì¼ í¬ê¸° í™•ì¸ìœ¼ë¡œ ì²˜ë¦¬ ì „ëµ ê²°ì •
                    file_size = os.path.getsize(file_path)
                    size_threshold_hash = self.config.get('processing', {}).get('size_threshold_hash', 50 * 1024)  # 50KB
                    size_threshold_mtime = self.config.get('processing', {}).get('size_threshold_mtime', 10 * 1024 * 1024)  # 10MB
                    
                    if file_size < size_threshold_hash:
                        # ì‘ì€ íŒŒì¼: í•­ìƒ í•´ì‹œ ê³„ì‚°
                        with open(file_path, 'rb') as f:
                            content = f.read()
                            current_hash = hashlib.sha256(content).hexdigest()
                        
                        if file_path not in existing_hashes or existing_hashes[file_path] != current_hash:
                            changed_files.append(file_path)
                            self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ì†Œí˜•): {file_path}")
                            
                    elif file_size < size_threshold_mtime:
                        # ì¤‘ê°„ í¬ê¸° íŒŒì¼: mtime ë¨¼ì € í™•ì¸ í›„ í•„ìš”ì‹œ í•´ì‹œ
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        
                        # ê¸°ì¡´ íŒŒì¼ì˜ mtimeê³¼ ë¹„êµ
                        existing_file = next((f for f in existing_files if f.path == file_path), None)
                        if not existing_file or existing_file.mtime < file_mtime:
                            # mtimeì´ ë‹¤ë¥´ë©´ í•´ì‹œ í™•ì¸
                            with open(file_path, 'rb') as f:
                                content = f.read()
                                current_hash = hashlib.sha256(content).hexdigest()
                            
                            if file_path not in existing_hashes or existing_hashes[file_path] != current_hash:
                                changed_files.append(file_path)
                                self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ì¤‘í˜•): {file_path}")
                        
                    else:
                        # ëŒ€í˜• íŒŒì¼: mtimeë§Œ í™•ì¸
                        file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                        existing_file = next((f for f in existing_files if f.path == file_path), None)
                        
                        if not existing_file or existing_file.mtime < file_mtime:
                            changed_files.append(file_path)
                            self.logger.debug(f"ë³€ê²½ëœ íŒŒì¼ ê°ì§€ (ëŒ€í˜•): {file_path}")
                    
                except Exception as e:
                    # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ í¬í•¨
                    self.logger.warning(f"íŒŒì¼ ë³€ê²½ ê°ì§€ ì‹¤íŒ¨ {file_path}: {e}")
                    changed_files.append(file_path)
            
        except Exception as e:
            self.logger.error("ì¦ë¶„ ë¶„ì„ íŒŒì¼ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜", exception=e)
            # ì˜¤ë¥˜ ì‹œ ì „ì²´ íŒŒì¼ ë°˜í™˜
            return source_files
        finally:
            session.close()
            
        return changed_files
    
    async def _handle_deleted_files(self, deleted_files: set, project_id: int):
        """ì‚­ì œëœ íŒŒì¼ë“¤ì˜ ë©”íƒ€ë°ì´í„° ì •ë¦¬"""
        
        self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì‹œì‘: {len(deleted_files)}ê°œ íŒŒì¼")
        
        try:
            # ë©”íƒ€ë°ì´í„° ì—”ì§„ì„ í†µí•´ ì‚­ì œ ì²˜ë¦¬
            cleaned_count = await self.metadata_engine.cleanup_deleted_files(deleted_files, project_id)
            
            # JSP/MyBatis íŒŒì„œì˜ ìºì‹œë„ ì •ë¦¬ (ìˆëŠ” ê²½ìš°)
            if 'jsp_mybatis' in self.parsers:
                for file_path in deleted_files:
                    try:
                        self.parsers['jsp_mybatis'].handle_deleted_file(file_path)
                    except Exception as e:
                        self.logger.warning(f"íŒŒì„œ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            
            self.logger.info(f"ì‚­ì œëœ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ ì •ë¦¬ë¨")
            
        except Exception as e:
            self.logger.error("ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜", exception=e)
            raise
        
    def _match_pattern(self, file_path: str, pattern: str) -> bool:
        """íŒŒì¼ íŒ¨í„´ ë§¤ì¹­ (ê°œì„ ëœ ë¡œì§)"""
        import fnmatch
        
        # ì ˆëŒ€ ê²½ë¡œì™€ ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ í™•ì¸
        abs_path = os.path.abspath(file_path)
        rel_path = os.path.relpath(file_path)
        basename = os.path.basename(file_path)
        
        return (fnmatch.fnmatch(abs_path, pattern) or 
                fnmatch.fnmatch(rel_path, pattern) or 
                fnmatch.fnmatch(basename, pattern))
        
    async def _generate_analysis_summary(self, project_id: int, 
                                       analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± (ê°œì„ ëœ í†µê³„)"""
        
        try:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì—”ì§„ ìš”ì•½
            engine_summary = await self.metadata_engine.generate_project_summary(project_id)
            
            # ì¶”ê°€ í†µê³„ ì •ë³´
            enhanced_summary = {
                **engine_summary,
                'processing_stats': {
                    'success_rate': (analysis_results['success_count'] / 
                                   max(1, analysis_results['total_files'])) * 100,
                    'error_rate': (analysis_results['error_count'] / 
                                 max(1, analysis_results['total_files'])) * 100,
                    'files_per_type': {
                        'java': analysis_results['java_files'],
                        'jsp': analysis_results['jsp_files'],
                        'xml': analysis_results['xml_files']
                    }
                },
                'quality_indicators': {
                    'avg_classes_per_java_file': (
                        analysis_results['total_classes'] / max(1, analysis_results['java_files'])
                    ),
                    'avg_methods_per_class': (
                        analysis_results['total_methods'] / max(1, analysis_results['total_classes'])
                    ),
                    'sql_units_found': analysis_results['total_sql_units']
                }
            }
            
            return enhanced_summary
            
        except Exception as e:
            self.logger.error("ë¶„ì„ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜", exception=e)
            return {
                'error': str(e),
                'basic_stats': {'files': 0, 'classes': 0, 'methods': 0, 'sql_units': 0},
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
            
    async def _analyze_files(self, source_files: List[str], project_id: int) -> Dict[str, Any]:
        """ì†ŒìŠ¤ íŒŒì¼ë“¤ ë¶„ì„"""
        
        analysis_results = {
            'total_files': len(source_files),
            'success_count': 0,
            'error_count': 0,
            'java_files': 0,
            'jsp_files': 0,
            'xml_files': 0,
            'total_classes': 0,
            'total_methods': 0,
            'total_sql_units': 0,
            'errors': []
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´
        max_workers = self.config['processing'].get('max_workers', 4)
        semaphore = asyncio.Semaphore(max_workers)
        
        # íŒŒì¼ë³„ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            self._analyze_single_file(file_path, project_id, semaphore) 
            for file_path in source_files
        ]
        
        # ë³‘ë ¬ ì‹¤í–‰
        file_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì§‘ê³„
        for i, result in enumerate(file_results):
            if isinstance(result, Exception):
                analysis_results['errors'].append({
                    'file': source_files[i],
                    'error': str(result)
                })
                analysis_results['error_count'] += 1
            else:
                analysis_results['success_count'] += 1
                
                # íŒŒì¼ íƒ€ì…ë³„ ì¹´ìš´íŒ…
                file_path = source_files[i]
                if file_path.endswith('.java'):
                    analysis_results['java_files'] += 1
                elif file_path.endswith('.jsp'):
                    analysis_results['jsp_files'] += 1
                elif file_path.endswith('.xml'):
                    analysis_results['xml_files'] += 1
                    
                # ìš”ì†Œë³„ ì¹´ìš´íŒ…
                if result:
                    analysis_results['total_classes'] += result.get('classes', 0)
                    analysis_results['total_methods'] += result.get('methods', 0)
                    analysis_results['total_sql_units'] += result.get('sql_units', 0)
                    
        return analysis_results
        
    async def _analyze_single_file(self, file_path: str, project_id: int, semaphore: asyncio.Semaphore) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        
        async with semaphore:
            try:
                self.logger.debug(f"íŒŒì¼ ë¶„ì„ ì‹œì‘: {file_path}")
                
                result = {'classes': 0, 'methods': 0, 'sql_units': 0}
                
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ íŒŒì„œ ì„ íƒ
                if 'java' in self.parsers and self.parsers['java'].can_parse(file_path):
                    file_obj, classes, methods, edges = self.parsers['java'].parse_file(file_path, project_id)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    await self.metadata_engine.save_java_analysis(file_obj, classes, methods, edges)
                    
                    result['classes'] = len(classes)
                    result['methods'] = len(methods)
                    
                elif 'jsp_mybatis' in self.parsers and self.parsers['jsp_mybatis'].can_parse(file_path):
                    file_obj, sql_units, joins, filters, edges, vulnerabilities = self.parsers['jsp_mybatis'].parse_file(file_path, project_id)
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    await self.metadata_engine.save_jsp_mybatis_analysis(file_obj, sql_units, joins, filters, edges, vulnerabilities)
                    
                    result['sql_units'] = len(sql_units)
                    
                return result
                
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
                raise
    
    def _validate_confidence_formula_on_startup(self):
        """
        ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ confidence formula ê²€ì¦ ìˆ˜í–‰
        ì •ì œì‹ ë¶„ì„ #22ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê¸°ëŠ¥
        """
        try:
            self.logger.info("Confidence formula ê²€ì¦ ì‹œì‘...")
            
            # Ground truth ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            if len(self.confidence_validator.ground_truth_data) == 0:
                self.logger.warning("Ground truth ë°ì´í„°ê°€ ì—†ìŒ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                return
            
            # Confidence formula ê²€ì¦ ì‹¤í–‰
            validation_result = self.confidence_validator.validate_confidence_formula()
            
            if 'mean_absolute_error' in validation_result:
                mae = validation_result['mean_absolute_error']
                self.logger.info(f"Confidence formula MAE: {mae:.3f}")
                
                # ì¡°ì • í•„ìš”ì„± í™•ì¸
                if mae > 0.10:  # 10% ì´ìƒ ì˜¤ë¥˜
                    self.logger.warning(f"Confidence ì˜¤ë¥˜ê°€ í¼: {mae:.1%} - ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê· ì¸")
                    self._attempt_confidence_calibration()
                else:
                    self.logger.info("í˜„ì¬ confidence formulaê°€ ì ì ˆí•¨")
            
        except Exception as e:
            self.logger.error(f"Confidence ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì‹œìŠ¤í…œ ì‹œì‘ì„ ë§‰ì§€ëŠ” ì•Šë„ë¡ í•¨
    
    def _attempt_confidence_calibration(self):
        """
        Confidence formula ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„
        """
        try:
            self.logger.info("ìë™ confidence ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„...")
            
            calibration_result = self.confidence_validator.calibrate_weights()
            
            if calibration_result.get('recommendation') == 'apply':
                improvement = calibration_result.get('improvement_percentage', 0)
                self.logger.info(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ {improvement:.1f}% ê°œì„  ì˜ˆìƒ")
                
                # ìë™ìœ¼ë¡œ ì ìš©
                if self.confidence_calibrator.apply_calibration(self.confidence_calculator):
                    self.logger.info("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ì ìš©ë¨")
                else:
                    self.logger.warning("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨")
            else:
                self.logger.info("í˜„ì¬ ê°€ì¤‘ì¹˜ê°€ ìµœì  ìƒíƒœ")
                
        except Exception as e:
            self.logger.error(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def generate_confidence_accuracy_report(self, output_path: str = None) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            output_path: ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì •í™•ë„ ë³´ê³ ì„œ ë°ì´í„°
        """
        try:
            self.logger.info("Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
            
            # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            report = self.confidence_validator.generate_confidence_accuracy_report()
            
            # íŒŒì¼ë¡œ ì €ì¥ (ì§€ì •ëœ ê²½ìš°)
            if output_path:
                import json
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                self.logger.info(f"Confidence ë³´ê³ ì„œ ì €ì¥ë¨: {output_path}")
            
            # ìš”ì•½ ì •ë³´ ë¡œê¹…
            if 'validation_report' in report:
                val_report = report['validation_report']
                if 'mean_absolute_error' in val_report:
                    mae = val_report['mean_absolute_error']
                    self.logger.info(f"Current confidence MAE: {mae:.3f}")
                    
                if 'error_distribution' in val_report:
                    error_dist = val_report['error_distribution']
                    excellent_pct = error_dist['excellent']['percentage'] * 100
                    poor_pct = error_dist['poor']['percentage'] * 100
                    self.logger.info(f"Error distribution: {excellent_pct:.1f}% excellent, {poor_pct:.1f}% poor")
            
            return report
            
        except Exception as e:
            self.logger.error(f"Confidence ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def add_ground_truth_entry(self, file_path: str, parser_type: str, 
                               expected_confidence: float, **kwargs) -> bool:
        """
        Ground truth ë°ì´í„° ì—”íŠ¸ë¦¬ ì¶”ê°€
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            parser_type: íŒŒì„œ íƒ€ì…
            expected_confidence: ê¸°ëŒ€ ì‹ ë¢°ë„ (0.0-1.0)
            **kwargs: ê¸°íƒ€ ground truth ë°ì´í„°
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            entry = GroundTruthEntry(
                file_path=file_path,
                parser_type=parser_type,
                expected_confidence=expected_confidence,
                expected_classes=kwargs.get('expected_classes', 0),
                expected_methods=kwargs.get('expected_methods', 0),
                expected_sql_units=kwargs.get('expected_sql_units', 0),
                verified_tables=kwargs.get('verified_tables', []),
                complexity_factors=kwargs.get('complexity_factors', {}),
                notes=kwargs.get('notes', ''),
                verifier=kwargs.get('verifier', 'manual')
            )
            
            self.confidence_validator.add_ground_truth_entry(entry)
            self.logger.info(f"Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ë¨: {file_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False

    def export_metadata_to_md(self, output_dir: str, project_name: str) -> Dict[str, Any]:
        """
        ë©”íƒ€ë°ì´í„°ë¥¼ Markdown íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
        
        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„
            
        Returns:
            ë‚´ë³´ë‚´ê¸° ê²°ê³¼ ì •ë³´
        """
        try:
            from datetime import datetime
            import os
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            session = self.db_manager.get_session()
            files = session.query(File).filter(File.project_name == project_name).all()
            
            if not files:
                return {
                    'success': False,
                    'error': f'í”„ë¡œì íŠ¸ "{project_name}"ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }
            
            created_files = []
            
            # 1. í”„ë¡œì íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
            summary_file = output_path / f"{project_name}_metadata_summary.md"
            self._create_summary_report(summary_file, project_name, files)
            created_files.append(str(summary_file))
            
            # 2. íŒŒì¼ë³„ ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
            details_dir = output_path / "file_details"
            details_dir.mkdir(exist_ok=True)
            
            for file_obj in files:
                detail_file = details_dir / f"{self._sanitize_filename(file_obj.relative_path)}.md"
                self._create_file_detail_report(detail_file, file_obj)
                created_files.append(str(detail_file))
            
            # 3. ì‹ ë¢°ë„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± (confidence ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
            confidence_file = output_path / f"{project_name}_confidence_analysis.md"
            self._create_confidence_report(confidence_file, project_name, files)
            created_files.append(str(confidence_file))
            
            session.close()
            
            return {
                'success': True,
                'files_created': len(created_files),
                'file_list': created_files
            }
            
        except Exception as e:
            self.logger.error(f"MD ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
        import re
        # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        safe_name = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ ë³€í™˜
        safe_name = re.sub(r'_{2,}', '_', safe_name)
        return safe_name.strip('_')
    
    def _create_summary_report(self, file_path: Path, project_name: str, files: list):
        """í”„ë¡œì íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        from collections import defaultdict
        
        # í†µê³„ ê³„ì‚°
        file_types = defaultdict(int)
        total_confidence = 0
        confidence_count = 0
        
        for file_obj in files:
            ext = Path(file_obj.relative_path).suffix
            file_types[ext] += 1
            
            if file_obj.confidence and file_obj.confidence > 0:
                total_confidence += file_obj.confidence
                confidence_count += 1
        
        avg_confidence = total_confidence / confidence_count if confidence_count > 0 else 0
        
        content = f"""# {project_name} ë©”íƒ€ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

## ìƒì„± ì •ë³´
- **ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **í”„ë¡œì íŠ¸ëª…**: {project_name}
- **ë¶„ì„ íŒŒì¼ ìˆ˜**: {len(files)}ê°œ

## íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„
"""
        
        for ext, count in sorted(file_types.items()):
            content += f"- **{ext or 'í™•ì¥ì ì—†ìŒ'}**: {count}ê°œ\n"
        
        content += f"""
## ì‹ ë¢°ë„ ë¶„ì„
- **í‰ê·  ì‹ ë¢°ë„**: {avg_confidence:.2%} ({confidence_count}ê°œ íŒŒì¼)
- **ì‹ ë¢°ë„ ë°ì´í„° ì—†ìŒ**: {len(files) - confidence_count}ê°œ íŒŒì¼

## íŒŒì¼ ëª©ë¡
| íŒŒì¼ ê²½ë¡œ | íƒ€ì… | ì‹ ë¢°ë„ | ìµœì¢… ì—…ë°ì´íŠ¸ |
|-----------|------|--------|---------------|
"""
        
        for file_obj in files:
            confidence_str = f"{file_obj.confidence:.2%}" if file_obj.confidence else "N/A"
            content += f"| {file_obj.relative_path} | {Path(file_obj.relative_path).suffix} | {confidence_str} | {file_obj.last_modified or 'N/A'} |\n"
        
        file_path.write_text(content, encoding='utf-8')
    
    def _create_file_detail_report(self, file_path: Path, file_obj):
        """ê°œë³„ íŒŒì¼ ìƒì„¸ ë³´ê³ ì„œ ìƒì„±"""
        content = f"""# íŒŒì¼ ë¶„ì„ ìƒì„¸ ë³´ê³ ì„œ

## íŒŒì¼ ì •ë³´
- **ê²½ë¡œ**: {file_obj.relative_path}
- **íŒŒì¼ íƒ€ì…**: {Path(file_obj.relative_path).suffix}
- **í¬ê¸°**: {file_obj.size} bytes
- **ìµœì¢… ìˆ˜ì •ì¼**: {file_obj.last_modified or 'N/A'}
- **í•´ì‹œ**: {file_obj.file_hash}

## ë¶„ì„ ê²°ê³¼
- **ì‹ ë¢°ë„**: {f"{file_obj.confidence:.2%}" if file_obj.confidence else "N/A"}
- **ìƒì„±ì¼ì‹œ**: {file_obj.created_at}
- **ì—…ë°ì´íŠ¸ì¼ì‹œ**: {file_obj.updated_at}

## ë©”íƒ€ë°ì´í„°
"""
        
        if hasattr(file_obj, 'metadata') and file_obj.metadata:
            content += f"```json\n{file_obj.metadata}\n```\n"
        else:
            content += "ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        file_path.write_text(content, encoding='utf-8')
    
    def _create_confidence_report(self, file_path: Path, project_name: str, files: list):
        """ì‹ ë¢°ë„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        confidence_files = [f for f in files if f.confidence and f.confidence > 0]
        
        if not confidence_files:
            content = f"""# {project_name} ì‹ ë¢°ë„ ë¶„ì„ ë³´ê³ ì„œ

ì‹ ë¢°ë„ ë°ì´í„°ê°€ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.
"""
        else:
            # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„ë¥˜
            high_conf = [f for f in confidence_files if f.confidence >= 0.8]
            med_conf = [f for f in confidence_files if 0.6 <= f.confidence < 0.8]
            low_conf = [f for f in confidence_files if f.confidence < 0.6]
            
            avg_conf = sum(f.confidence for f in confidence_files) / len(confidence_files)
            
            content = f"""# {project_name} ì‹ ë¢°ë„ ë¶„ì„ ë³´ê³ ì„œ

## ì „ì²´ í†µê³„
- **ì‹ ë¢°ë„ ë°ì´í„° íŒŒì¼**: {len(confidence_files)}ê°œ
- **í‰ê·  ì‹ ë¢°ë„**: {avg_conf:.2%}

## ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„ë¥˜
- **ë†’ì€ ì‹ ë¢°ë„ (80% ì´ìƒ)**: {len(high_conf)}ê°œ
- **ì¤‘ê°„ ì‹ ë¢°ë„ (60% ì´ìƒ)**: {len(med_conf)}ê°œ  
- **ë‚®ì€ ì‹ ë¢°ë„ (60% ë¯¸ë§Œ)**: {len(low_conf)}ê°œ

## ë‚®ì€ ì‹ ë¢°ë„ íŒŒì¼ ëª©ë¡
"""
            
            if low_conf:
                content += "| íŒŒì¼ ê²½ë¡œ | ì‹ ë¢°ë„ |\n|-----------|--------|\n"
                for file_obj in sorted(low_conf, key=lambda x: x.confidence):
                    content += f"| {file_obj.relative_path} | {file_obj.confidence:.2%} |\n"
            else:
                content += "ë‚®ì€ ì‹ ë¢°ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\n"
        
        file_path.write_text(content, encoding='utf-8')

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(
        description='ì†ŒìŠ¤ ì½”ë“œ ë¶„ì„ ë„êµ¬ - 1ë‹¨ê³„ ë©”íƒ€ì •ë³´ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ ì‚¬ìš©ë²•:
  python main.py PROJECT/sample-app
  python main.py PROJECT/sample-app --project-name "ìƒ˜í”Œ ì•±"
  python main.py PROJECT/sample-app --config custom_config.yaml
        """
    )
    
    parser.add_argument('--input-path', help='ë¶„ì„í•  í”„ë¡œì íŠ¸ ì†ŒìŠ¤ ê²½ë¡œ (ê¸°ë³¸ê°’: ./PROJECT/sampleSrc/src)')
    parser.add_argument('--config', default='./config/config.yaml', 
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ./config/config.yaml)')
    parser.add_argument('--project-name', help='í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: input-pathì˜ ë§ˆì§€ë§‰ í´ë”ëª…)')
    parser.add_argument('--all', action='store_true',
                       help='ëª¨ë“  ë¶„ì„ ë° ì‹œê°í™” ì‘ì—… ì‹¤í–‰ (Quick Startìš©)')
    parser.add_argument('--incremental', action='store_true', 
                       help='ì¦ë¶„ ë¶„ì„ ëª¨ë“œ (ë³€ê²½ëœ íŒŒì¼ë§Œ ë¶„ì„)')
    parser.add_argument('--include-ext', help='í¬í•¨í•  íŒŒì¼ í™•ì¥ì ëª©ë¡(ì½¤ë§ˆ êµ¬ë¶„). ì˜ˆ: .java,.jsp,.xml')
    parser.add_argument('--include-dirs', help='í¬í•¨í•  í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡(ì½¤ë§ˆ êµ¬ë¶„). ì˜ˆ: src/main/java,src/main/webapp')
    parser.add_argument('--verbose', '-v', action='store_true', 
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    parser.add_argument('--quiet', '-q', action='store_true', 
                       help='ìµœì†Œ ë¡œê·¸ ì¶œë ¥')
    
    # Confidence validation ê´€ë ¨ ì˜µì…˜
    parser.add_argument('--validate-confidence', action='store_true',
                       help='Confidence formula ê²€ì¦ ì‹¤í–‰')
    parser.add_argument('--calibrate-confidence', action='store_true', 
                       help='Confidence formula ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜')
    parser.add_argument('--confidence-report', metavar='OUTPUT_FILE',
                       help='Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± (íŒŒì¼ ê²½ë¡œ ì§€ì •)')
    parser.add_argument('--add-ground-truth', nargs=3, 
                       metavar=('FILE_PATH', 'PARSER_TYPE', 'EXPECTED_CONFIDENCE'),
                       help='Ground truth ë°ì´í„° ì—”íŠ¸ë¦¬ ì¶”ê°€')
    parser.add_argument('--export-md', metavar='OUTPUT_DIR', 
                       help='ë©”íƒ€ë°ì´í„°ë¥¼ Markdown íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •)')
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²½ë¡œ ë° í”„ë¡œì íŠ¸ ì´ë¦„ ì²˜ë¦¬
    if args.input_path:
        project_root = args.input_path
    else:
        project_root = "./PROJECT/sampleSrc/src" # ê¸°ë³¸ê°’

    if not args.project_name:
        # input_pathì˜ ë§ˆì§€ë§‰ í´ë”ëª…ì„ í”„ë¡œì íŠ¸ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        project_name = os.path.basename(os.path.abspath(project_root))
    else:
        project_name = args.project_name

    # ì…ë ¥ ê²€ì¦
    if not os.path.exists(project_root):
        print(f"âŒ ì˜¤ë¥˜: í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_root}")
        sys.exit(1)
        
    if not os.path.exists(args.config):
        print(f"âŒ ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {args.config}")
        sys.exit(1)
        
    try:
        # ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("ì†ŒìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        analyzer = SourceAnalyzer(args.config)
        
        # ë¡œê·¸ ë ˆë²¨ ì¡°ì • (ë¡œê±°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¡œê¹… ì‚¬ìš©)
        if hasattr(analyzer.logger, 'logger'):
            if args.verbose:
                analyzer.logger.logger.setLevel('DEBUG')
            elif args.quiet:
                analyzer.logger.logger.setLevel('WARNING')
        
        # Validation ê´€ë ¨ ëª…ë ¹ì–´ ì²˜ë¦¬
        validation_performed = False
        
        # Ground truth ë°ì´í„° ì¶”ê°€
        if args.add_ground_truth:
            file_path, parser_type, expected_conf = args.add_ground_truth
            try:
                expected_confidence = float(expected_conf)
                if analyzer.add_ground_truth_entry(file_path, parser_type, expected_confidence):
                    print(f"âœ… Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ë¨: {file_path}")
                else:
                    print(f"âŒ Ground truth ì—”íŠ¸ë¦¬ ì¶”ê°€ ì‹¤íŒ¨: {file_path}")
                validation_performed = True
            except ValueError:
                print(f"âŒ ì˜¤ë¥˜: ì˜ëª»ëœ confidence ê°’: {expected_conf}")
        
        # Confidence ê²€ì¦
        if args.validate_confidence:
            print("ğŸ” Confidence formula ê²€ì¦ ì‹¤í–‰ ì¤‘...")
            validation_result = analyzer.confidence_validator.validate_confidence_formula()
            
            if 'mean_absolute_error' in validation_result:
                mae = validation_result['mean_absolute_error']
                print(f"ğŸ“Š Confidence MAE: {mae:.3f}")
                
                if 'error_distribution' in validation_result:
                    error_dist = validation_result['error_distribution']
                    excellent_pct = error_dist['excellent']['percentage'] * 100
                    poor_pct = error_dist['poor']['percentage'] * 100
                    print(f"ğŸ“ˆ ì˜¤ë¥˜ ë¶„í¬: {excellent_pct:.1f}% ìš°ìˆ˜, {poor_pct:.1f}% ë¶ˆëŸ‰")
            validation_performed = True
        
        # Confidence ìº˜ë¦¬ë¸Œë ˆì´ì…˜
        if args.calibrate_confidence:
            print("ğŸ”§ Confidence formula ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
            calibration_result = analyzer.confidence_validator.calibrate_weights()
            
            if calibration_result.get('recommendation') == 'apply':
                improvement = calibration_result.get('improvement_percentage', 0)
                print(f"ğŸš€ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ {improvement:.1f}% ê°œì„  ê°€ëŠ¥")
                
                if analyzer.confidence_calibrator.apply_calibration(analyzer.confidence_calculator):
                    print("âœ… ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì™„ë£Œ")
                else:
                    print("âŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© ì‹¤íŒ¨")
            else:
                print("âœ¨ í˜„ì¬ ê°€ì¤‘ì¹˜ê°€ ì´ë¯¸ ìµœì  ìƒíƒœ")
            validation_performed = True
        
        # Confidence ë¦¬í¬íŠ¸ ìƒì„±
        if args.confidence_report:
            print(f"ğŸ“„ Confidence ì •í™•ë„ ë³´ê³ ì„œ ìƒì„± ì¤‘: {args.confidence_report}")
            report = analyzer.generate_confidence_accuracy_report(args.confidence_report)
            
            if 'error' not in report:
                print(f"âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {args.confidence_report}")
                
                # ìš”ì•½ ì •ë³´ ì¶œë ¥
                if 'validation_report' in report and 'mean_absolute_error' in report['validation_report']:
                    mae = report['validation_report']['mean_absolute_error']
                    print(f"ğŸ“Š í˜„ì¬ ì˜¤ë¥˜ìœ¨: {mae:.1%}")
            else:
                print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {report['error']}")
            validation_performed = True
        
        # Validation ëª…ë ¹ë§Œ ì‹¤í–‰í•œ ê²½ìš° ì—¬ê¸°ì„œ ì¢…ë£Œ
        if validation_performed:
            print("\nğŸ¯ Validation ì‘ì—… ì™„ë£Œ")
            return
        
        # í”„ë¡œì íŠ¸ ë¶„ì„ ì‹¤í–‰
        print(f"í”„ë¡œì íŠ¸ ë¶„ì„ ì‹œì‘: {args.project_path}")
        # ëŸ°íƒ€ì„ íŒŒì¼ íŒ¨í„´ ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        if args.include_ext or args.include_dirs:
            fps = analyzer.config.setdefault('file_patterns', {})
            include: List[str] = []
            if args.include_ext:
                for ext in [x.strip() for x in args.include_ext.split(',') if x.strip()]:
                    if not ext.startswith('.'):
                        ext = '.' + ext
                    include.append(f"**/*{ext}")
            if args.include_dirs:
                for d in [x.strip() for x in args.include_dirs.split(',') if x.strip()]:
                    include.extend([f"{d}/**/*.java", f"{d}/**/*.jsp", f"{d}/**/*.xml"])
            if include:
                fps['include'] = include

        result = asyncio.run(analyzer.analyze_project(
            project_root, 
            project_name,
            args.incremental
        ))
        
        # ê²°ê³¼ ì¶œë ¥
        print_analysis_results(result)
        
        # ë©”íƒ€ë°ì´í„° MD ë‚´ë³´ë‚´ê¸° ì²˜ë¦¬
        if args.export_md:
            print(f"\nğŸ“„ ë©”íƒ€ë°ì´í„°ë¥¼ Markdownìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ì¤‘: {args.export_md}")
            try:
                export_result = analyzer.export_metadata_to_md(args.export_md, project_name)
                if export_result.get('success'):
                    print(f"âœ… Markdown ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_result['files_created']}ê°œ íŒŒì¼ ìƒì„±")
                    for file_path in export_result.get('file_list', []):
                        print(f"   - {file_path}")
                else:
                    print(f"âŒ Markdown ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {export_result.get('error', 'Unknown error')}")
            except Exception as e:
                print(f"âŒ Markdown ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)


def print_analysis_results(result: Dict[str, Any]):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    
    print("\n" + "=" * 70)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"í”„ë¡œì íŠ¸: {result['project_name']}")
    print(f"ê²½ë¡œ: {result['project_root']}")
    print(f"ë¶„ì„ ì‹œê°„: {result['analysis_time']:.2f}ì´ˆ")
    print()
    
    # íŒŒì¼ ì²˜ë¦¬ í˜„í™©
    results = result['analysis_results']
    print("íŒŒì¼ ì²˜ë¦¬ í˜„í™©:")
    print(f"  - ì „ì²´ íŒŒì¼: {result['files_analyzed']}ê°œ")
    print(f"  - ì„±ê³µ: {results['success_count']}ê°œ")
    print(f"  - ì‹¤íŒ¨: {results['error_count']}ê°œ")
    if results['error_count'] > 0:
        print(f"  - ì„±ê³µë¥ : {(results['success_count']/results['total_files']*100):.1f}%")
    print()
    
    # íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„
    print("íŒŒì¼ íƒ€ì…ë³„ ë¶„ì„:")
    print(f"  - Java íŒŒì¼: {results['java_files']}ê°œ")
    print(f"  - JSP íŒŒì¼: {results['jsp_files']}ê°œ")
    print(f"  - XML íŒŒì¼: {results['xml_files']}ê°œ")
    print()
    
    # ë¶„ì„ ê²°ê³¼
    print("ë¶„ì„ ê²°ê³¼:")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {results['total_classes']}ê°œ")
    print(f"  - ë©”ì„œë“œ ìˆ˜: {results['total_methods']}ê°œ")
    print(f"  - SQL êµ¬ë¬¸ ìˆ˜: {results['total_sql_units']}ê°œ")
    print()
    
    # í’ˆì§ˆ ì§€í‘œ
    if 'quality_indicators' in result['summary']:
        quality = result['summary']['quality_indicators']
        print("í’ˆì§ˆ ì§€í‘œ:")
        print(f"  - Java íŒŒì¼ë‹¹ í‰ê·  í´ë˜ìŠ¤ ìˆ˜: {quality['avg_classes_per_java_file']:.1f}ê°œ")
        print(f"  - í´ë˜ìŠ¤ë‹¹ í‰ê·  ë©”ì„œë“œ ìˆ˜: {quality['avg_methods_per_class']:.1f}ê°œ")
        print(f"  - ë°œê²¬ëœ SQL êµ¬ë¬¸: {quality['sql_units_found']}ê°œ")
        print()
    
    # ì˜¤ë¥˜ ì •ë³´ (ìˆëŠ” ê²½ìš°)
    if results['errors']:
        print("ì˜¤ë¥˜ ìƒì„¸:")
        for i, error in enumerate(results['errors'][:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            if isinstance(error, dict) and 'file' in error:
                print(f"  {i+1}. {error['file']}: {error['error']}")
            else:
                print(f"  {i+1}. {error}")
        
        if len(results['errors']) > 5:
            print(f"  ... ë° {len(results['errors'])-5}ê°œ ì¶”ê°€ ì˜¤ë¥˜")
        print()
    
    # ì„±ëŠ¥ ìš”ì•½
    if result['files_analyzed'] > 0 and result['analysis_time'] > 0:
        files_per_sec = result['files_analyzed'] / result['analysis_time']
        print(f"ì„±ëŠ¥: {files_per_sec:.1f}íŒŒì¼/ì´ˆ")
    
    print("=" * 70)


if __name__ == "__main__":
    main()
